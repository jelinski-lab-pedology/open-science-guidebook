[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Open Science: Tools Applications, & Workflow",
    "section": "",
    "text": "Preface\nThis book outlines an open, standardized workflow for use on all projects in the Jelinski lab. With the diverse library of tools, apps, and digital repositories available to you as a researcher, at times, it is extremely difficult to understand which ones to use and how to put together a coherent workflow that promotes the FAIR (Findable, Accesible, Interoperable, and Reusable) and FAIRER (Findable, Accesible, Interoperable, Reusable, Ethical, and Reproducible) open science principles in your work. My goal is to standardize workflow for all projects in our lab - or at the very least to provide a scaffolded, foundational approach that everyone can borrow and modify from as best fits their needs. This is not so much meant to force you into a particular way of doing your thinking things, but rather to free you up to do what’s really important, the research. Making decisions about how to name files, what format and style to use for code, how to structure project files and folders, and what tools and apps to use is exhausting, and these decisions take you away from the science. The goal of this standardized workflow is to produce open science that is easily reproducible, transparent, usable by others, and most importantly well organized and documented so that you and others can easily understand what you’ve done and how you’ve done it. There are a few principal concepts behind this workflow and how you can structure your work to be most productive:"
  },
  {
    "objectID": "index.html#no-work-is-wasted-work.",
    "href": "index.html#no-work-is-wasted-work.",
    "title": "Open Science: Tools Applications, & Workflow",
    "section": "No work is wasted work.",
    "text": "No work is wasted work.\nA major part of research is hitting dead ends, going down rabbit holes, and generally feeling like you might be wasting your time because it’s taking you so long to get to the correct analysis or perspective. However, none of this is waste of time. Every dead end is a door that has closed to allow you to find your path forward. It’s your job as a scientist and researcher to define and document those dead ends and not give up. Recognize that those dead ends are part of the process, and make those dead ends work for you. By using a standardized workflow even those dead ends become reproducible transparent learning experiences that are documented fully for you and others to grow from."
  },
  {
    "objectID": "index.html#reduce-trivial-decisions-so-that-you-can-focus-on-science-your-life-and-your-whole-being.",
    "href": "index.html#reduce-trivial-decisions-so-that-you-can-focus-on-science-your-life-and-your-whole-being.",
    "title": "Open Science: Tools Applications, & Workflow",
    "section": "Reduce trivial decisions so that you can focus on science, your life, and your whole being.",
    "text": "Reduce trivial decisions so that you can focus on science, your life, and your whole being.\nOne of my mentors as probationary faculty, Dr. Ed Nater, told me that a career in science “is a marathon, not a sprint”. Despite the modern urgency to publish as many papers as possible in a short amount of time, research still takes time. And often we don’t know exactly how long it is gonna take. It’s better to do good science then to do fast science. Of course ideally we would do good, fast science. This workflow will help you do that - it will help you do good science, create transparent, citeable and reproducible content, even from your unfinished drafts, dead ends, and false starts.\nAdditionally, because research is a marathon and not a sprint, it is absolutely important that you take care of your whole being. This means taking care of your body and your mind. Let me tell you a story.\nAs an academic, most of us who are in this for the career work long hours. I’ve been working long hours and overnights for as long as I can remember - this is normal in academia. I used to view this as “the grind”. I used to think that the answer to be more productive was just putting in more hours in the grind. If I could just stay up one more night, push through for three more hours, sit at my desk for 10-12 hours in a day, and work work work, I could produce more. What I realized as I’ve reached the middle of my career is that there are better ways to produce more which also allow you to be healthier than just grinding the large number of hours. Certainly hours are important, for example you probably are not gonna publish a research paper, write a thesis, write a dissertation, or be successful in a faculty job without putting in more than 40 hours a week and some overnights. But, those should not be the norm. There are ways to design your life so that those are few and far between. Striving for a physical and mental balance is extremely important and my years of grinding have taken a toll. Although they have given me a lot, and I’ve been able to achieve tenure, looking back I now see how I could have changed my processes to work fewer hours, be more physically healthy, more mentally available for my friends and family, and produce more.\nMuch of this starts with prioritizing your physical health. You simply can’t reach your top productivity if you are not physically healthy because the mind-body connection is strong. Physical health is a cornerstone for long-term productivity in any career. And science is no different. After years of mucking through different processes, workflows, and trial and error, I’m finally starting to reach an understanding regarding how to construct these processes to reduce decision fatigue and be more productive, allow me more time to do more things outside of work and to take care of myself my family. This all involves standardization. Jocko Willink is famous for his mantra “Discipline Equals Freedom”. This mantra also holds true in creative work such as academia. Although it seems counterintuitive and potentially restricting at first, standardized processes actually allow you to do better, more creative, more reproducible, and more creative and inspiring work. &gt; All it takes to make creativity a part of your life is the willingness to make it a habit. It is the product of preparation and effort, and is within reach of everyone. - Twyla Tharp\nThis is because all of the small decisions that you make about, for example (just to name a few):\n\nWhat to name files and how to create a structure in a folder or set of folders,\nwhat app to use to do data analysis,\nhow to write and structure code,\nwhat to do next when you have 15 minutes in your work schedule,\n\nreduce your ability to focus on what really matters, generating data, analyzing data, the science and the writing. By standardizing workflows and processes, we relegate the trivial stuff to a trusted system This system is one that holds important information, and allows you to access it freely and readily, but makes all of the trivial decisions for you. This allows you to focus on just the information. It frees you to think more and more deeply about what really matters, to focus your time and effort outside of work on your physical and mental health."
  },
  {
    "objectID": "index.html#implementing-a-standardized-workflow-is-just-good-science---and-open-science-is-the-future-and-the-future-is-here-now.",
    "href": "index.html#implementing-a-standardized-workflow-is-just-good-science---and-open-science-is-the-future-and-the-future-is-here-now.",
    "title": "Open Science: Tools Applications, & Workflow",
    "section": "Implementing a standardized workflow is just good science - and Open Science is the future (and the future is here now).",
    "text": "Implementing a standardized workflow is just good science - and Open Science is the future (and the future is here now).\n10 years ago, very few people were doing open science. Now with openscapes and a handful of other strong initiatives more and more people are doing open science in environmental fields. More journals require that you submit raw data, code or somehow package everything to make your analysis and results reproducible. This workflow will allow you to do that automatically as you build your project. When you get to the point of submitting a manuscript you will already have a citable online repository that you can point to when you submit your manuscript. This workflow also fosters a mindset of development and growth, where you “show your work” all the time. Showing your work all the time is a great way to:\n\nFoster collaboration,\nto be “constantly producing”, and\n\nto feel more accomplished and feel that you can be “done” at the end of a workday day.\n\nIn the beginning you may find implementing this workflow to be slower, and at first there will be a learning curve. However after 18 years of managing projects and doing science, I can tell you the time effort and mental strain saved from having an organized, standardized, and open process and workflow will save you an incredible amount of time, but it can also provide you a strong foundation that you can carry into the future and never duplicate or backtrack on work you have already done. I can’t tell you how many times I’ve gone back to read a paper that I’ve read five times just to fit it into another reference, or to format a bibliography, or to make sense of code that I wrote just six months ago, or to update the class syllabus documents or find different versions of files that are scattered across my computer, in cloud storage, and named different things. As you go on in your research career you will accumulate more and more of this baggage. If you start with a standardized workflow early in your career, this baggage will be manageable, well structured, available to you and others, and will actually promote a positive feedback loop whereby the work that you do now makes future work even easier and more creative."
  },
  {
    "objectID": "index.html#traveling-together",
    "href": "index.html#traveling-together",
    "title": "Open Science: Tools Applications, & Workflow",
    "section": "Traveling together",
    "text": "Traveling together\nI am a traveler on this road as well. I do not have all of the answers, but I have spent time developing a system and set of tools that have begun to pay huge dividends in my work and personal life. I hope to share these lessons learned with you.\nI hope you will join me on this journey."
  },
  {
    "objectID": "01-intro.html#workflow-stages",
    "href": "01-intro.html#workflow-stages",
    "title": "Introduction",
    "section": "Workflow Stages",
    "text": "Workflow Stages\nThe sections of this book each outline a single component of an integrated workflow that attempts at every turn to use well-documented, open source tools. The major components of this workflow are:\n\nProject Setup\nThis includes deciding what constitutes a project (seems like it should be an easy thing, but it’s actually somewhat difficult), and how to use a template to generate a file and folder structure for your project before you even begin that is deeply integrated across platforms. This also includes understand what “type” of project yours is - as this dictates how you will set the project up and collect and curate data.\nOnce you have your project defined and set up along with the folder structure across platforms, the next step is to begin producing documentation for your project. This is called project metadata and it is extremely important. Your project metadata gives you and others a general idea of the scope of the project, the motivation behind the project, where is going, when it is happening, and where the finals results and data will live.\nNot all projects that you begin will get to some final stage, however all projects should have a project structure and metadata even if these are empty. In fact, setting up a project with even just a trivial amount of meta-data and data structure encourages you to work more easily on that project in the future. This includes creating a badge or sticker that defines and brands each project. Although this might seem completely trivial, it actually helps you to view your project as an entity that is ready to be discussed and shared - even in its infancy by others, by your collaborators, or by your peers. It’s a fun way to stick your claim on a piece of ground that you are working on. There’s a whole section in this book on how to design and create a project sticker.\n\n\n“Pre”-Data\nBefore you start collecting data, and before you get to the point of looking at your raw data that you have collected, it’s important to set up pre-processes that allow you to standardize your data input and make it more easily retrievable in the future. This pre-data workflow include setting up a survey 123 app for data collection for field projects, designing and setting up data sheets, and for non-field projects, setting up meta-analysis structures or data forms.\n\n\nRaw Data Collection\nIn this stage, you generate your raw data. As you collect your data, you will use your pre-data structure and tools ensure that all of your raw data is captured, archived, and organized as you are doing it. This is ensures the highest quality data and also serves as an insurance policy against data loss. Avoiding unlabeled bags, pictures, or data sheets or notebooks that doesn’t contain the relevant information. Your pre-data steps and pre-data structure will ensure that this doesn’t happen.\n\n\nData Munging\nIn the data munging stage do you conduct QAQC and further curate your raw data, in a transparent and reproducible manner, and make decisions about how to best process your raw data to feed into your data analysis stage.\n\n\nData Analysis and Interpretation\nThe data analysis stage is probably the longest stage, and in fact it may start before you even have all of your data. The data analysis stage involves taking your pre-processed (“munged”) data and doing analysis on it in a transparent and reproducible way. This means that you will need to learn how to code. In my lab group we primarily use R. R is extremely flexible and widely used in soil science. Other scientific fields prefer python, and you may have a project where a different programming language makes more sense. That’s OK. But if you don’t know where to begin or what do use, I would suggest R. The data analysis step ends when you have produced tables or figures that allow you to begin interpreting your data and your results. Note that your tables and figures may not be final for a very long time - and you will likely be engaged in cycles of data analysis, writing, and further data analysis as you refine your interpretations. But it’s important to start somewhere and if you document your data analysis in a reproducible and transparent way, you won’t duplicate efforts down the road and will learn from all of your dead ends, particularly when it comes to coding.\nSetting poorly written or poorly managed code aside for even a week or two can cost you many hours when you come back to it. The goal is to have everything so well documented that you can hop back into a project and within 30 minutes be moving along at the place where you left off. This involves documenting many stages of your data analysis, which can see sometimes to slow you down while you are doing it. However, I can conifdently say that any time spent documenting and logging during data analysis is time saved by an order of magnitude when you inevitably are going back for the 20th 30th or 40th time into your scripts into your data and trying to make sense at all. Do it right the first time and even your dead ends will remain learning experiences that are well documented and allow you to move forward in a different path with your analysis.\n\n\nWriting\nThis section is about both the tools that we use for writing, and the philosophy of writing itself. Writing can be one of the most challenging parts of the job. It doesn’t have to be. We know so much about the philosophy of writing, and there are many tools to improve our writing and productivity. We can all learn to be better writers, we can all be more productive writers, and this aims to help you understand the philosophy of getting there. But in more practical sense, I hope to give you some tools that allow you to write in a more reproducible and transparent way to reduce writing and work stress. Part of that means being more efficient at writing down and logging everything.\nThis can be accomplished through the use of a personal knowledge management system (PKM). My system of choice is Obsidian (digital) and a Bullet Journal (analog). In addition to a PKM, getting in the habit of documenting writing notes and ensuring long-term viability of the notes that you created is extremely important. It means you will write faster, because you will never start with a blank page again. You always start writing with a structured outline text that was generated in the past, notes, thoughts or other things that have been all consolidated and maintained in your PKM - a trusted system where you don’t have to worry about losing thoughts or progress and is available to you whenever you choose to start working on writing in your project.\nThe reality is writing is not a discrete stage of a project that starts after data analysis. In this workflow writing begins with project definitions and project metadata and it doesn’t stop until the project or manuscript is submitted. Writing is continuous - you will revise things 20 or 30 times. Coding is also writing - in fact the best examples of well written code mean making that code not just readable for machines but also humans.\nMost science is collaborative, and nowadays writing will occur likely with collaborators - at a minimum me as your advisor, a group of scientists on a small project, or a large group of people working on a large project. It’s important to manage tools for version control, tools for collaboration in real time, and tools for rendering your documents so that you don’t have to spend time formatting. Formatting can take forever, but with modern tools like markdown language, publishing tools like Quarto, we can create from the same plain text document beautiful websites which can be referenced and cited by doi, we can create journal article submissions in PDF form that include all tables and figures, and we can house that same PKM system, which allows it to be searchable and usable to build on in other projects in the future.\nNOTE: these discrete project stages overklap significantly. Except for project definition and project management, for example. Writing and data analysis often take place at the same time because we “write to learn”.\nTODO: how to take notes (strategies - smart notes), revisit and improve your metadata, writing philosophies and writing to learn.\nNOTE: keep a data analysis log - keep a writing log\n\n\nLong term data archiving and storage\nGitHub is the main repository for code and data from projects in this lab, but also any data generated including spatial data for should go into a major database either hosted or sponsored by a stable entity. Long-term archiving also includes packaging up your entire project structure, which now should be standardized based on this lab work flow, so anybody in our lab, or anybody reading this documentation knows exactly what each folder means where to find certain files how to run code, even 10 or 20 years into the future. This workflow is designed to be as future proof as possible.\nNote that although these major components are listed in chronological order, they will likely not necessarily occur in distinct stages in your project. For example data analysis may begin with preliminary data and continue as you are writing drafts of manuscripts, AND writing occurs across all of these components - writing actually begins with project setup as you will see. If you have a project idea, you can start writing! This allows you to never start with a blank page\nTo execute this standardized workflow as described, it will be necessary for you to be familiar with and utilize the following tools:\n\nArcGIS Online: Survey123 (Proprietary - Free account available through UMN)\nObsidian (Open source)\nZotero (Open source)\nQGIS (Open source)\nR/RStudio (Open source)\nGitHub/GitHub Desktop (Proprietary - Free account available to anyone)\nGoogle Drive (Proprietary - Free account available to anyone, unlimited storage available through UMN)\nInkScape (Open source)\nHack MD (Proprietary - Free account available to anyone)\nnote to potentially include social professional sites like ResearchGate and Google Scholar + ORCID + X?\n\nThese are not a randomly selected group of tools - they are designed at every turn to allow deep integration and (as much as possible) free and open source usage. Although not all of these tools are open source (ArcGIS Online, GitHub, Google Drive, Hack MD) with the exception of ArcGIS Online, all of these tools have a free base level, which will be sufficient to execute this workflow. Additionally, because of UMN’s enterprise license with ArcGIS Online, all of those tools are available free to you if you are in our graduate or undergraduate programs.\nThese tools will require you to be fluent in the following languages:\n\nR (core language, critical skill)\nMarkdown (core language, critical skill)\nAlthough not required to execute this workflow, familiarity with Python, Javascript, and SQL might help you do more customization"
  },
  {
    "objectID": "02-project-setup.html#project-setup",
    "href": "02-project-setup.html#project-setup",
    "title": "Project Setup and Documentation",
    "section": "Project Setup",
    "text": "Project Setup\nThis includes deciding what constitutes a “project” (this seems like it should be an easy thing, but it’s actually very difficult), how to generate a file and folder structure for your project before you even begin that lives and syncs across platforms. This setup is designed to work for all of the project types we deal with regardless of the initial data origin (i.e. field projects, laboratory projects, review or data compilation).\n\n\n\n\n\n\nRequired Tools - Project Setup\n\n\n\n\nA project idea\nLocal folder structure on your personal computer\nGitHub/GitHub Desktop\nGoogle Drive\nZotero\nObsidian\nInkscape"
  },
  {
    "objectID": "02a-para-system.html#the-para-system-in-practice",
    "href": "02a-para-system.html#the-para-system-in-practice",
    "title": "1  Overview of the PARA system",
    "section": "1.1 The PARA system in practice",
    "text": "1.1 The PARA system in practice\nTiago Forte’s PARA acronym stands for: Projects, Areas, Resources, and Archive. This system is not strictly hierarchical, but each category from front to back in PARA is given precedence over the last. That is because the underlying philosophy is that projects should produce end outputs or deliverables. Things that are not yet projects, or are larger than a single project are more nebulous, but not any less valuable.\nUsing the PARA system requires you to set up high level folders (I would suggest in the Documents directory of your computer) for each of the PARA categories: Projects, Areas, Resources, and Archive. Then, all of your professional work, and eventually some of your personal work and interests will nest under these.\nDeciding what is a “Project” vs an “Area” vs a “Resource” vs an “Archive” is probably the most important part of this workflow and drives everything else, so I will start there. One simple way to think about this is that when you need to consider where something should go in this system start at the beginning - if its doesn’t fit into the definition of a “Project”, then move down and see if it fits into the definition of an “Area”. If it is not an “Area”, then it will likely be a “Resource” unless it is something completed or inactive, in which case it goes in the “Archive”."
  },
  {
    "objectID": "02a-para-system.html#what-is-a-project-how-should-i-name-my-projects",
    "href": "02a-para-system.html#what-is-a-project-how-should-i-name-my-projects",
    "title": "1  Overview of the PARA system",
    "section": "1.2 What is a Project? How should I name my Projects?",
    "text": "1.2 What is a Project? How should I name my Projects?\n\n1.2.1 Defining Projects - How to Determine if Something is a Project\nThis seems like a trivial question but defining project boundaries in research can be difficult. Where does one project end and another begin? Does a project include multiple sub projects?\nA project is something you are involved in that has a discrete, definable end state or product. Note that our definition of projects here means they are flat. There should not be multiple projects nested under other projects. Each project is a discrete entity - this aligns with GitHub repository organization (as we will see in future chapters), which allows deep integration. In our line of work, examples would be:\n\nA thesis chapter; see below for why a thesis or dissertation itself is actually larger than a Project - it is an Area\nA manuscript\nA proposal\nConference presentations and invited lectures (one-time - not recurring teaching responsibilities)\nReports to stakeholders on unpublished data. NOTE: grant reports (as addressed below) are not projects, but belong in areas with grants.\nNOTE: A grant is best defined as an Area (because these days all grants are collaborative) with multiple projects underneath. Also, because most grants actually have multiple end products such as reports and manuscripts.\n\n\n\n1.2.2 Naming Projects\nThis is a general style guide for naming projects (in the context of files or folder structures). Projects may have very long actual names (or you may not even know what, exactly the name of your project or project idea will eventually be). This doesn’t matter for the purposes of project setup. For now, we just need a two to four word abbreviated title for the project that we will use in our folder structure.\n\n1.2.2.1 Naming Manuscripts - standardized with unique IDs as these will become GitHub repos\nAll manuscripts (following project setup) will eventually have a GitHub repo which is built from the jelinski-lab-pedology-MXXX-project-template repo, and therefore coordinated and standardized naming is essential. Here is a style guide for naming your projects:\n\nUse all lowercase letters and short dashes instead of spaces.\nManuscripts should be designated as M (for manuscript, including thesis chapters, which should each be a publishable unit).\nProjects should then be given a three digit number (with two leading zeros for any project numbers under 100) - these are just given in sequence as we create projects - the order doesn’t matter. **If you are in my lab group*, please obtain a new project ID number by adding your project as a new row to (jelinski-lab-project-registry?). Simply add a row with the next number in sequence (i.e. if the last registered project is M059-jelinski-permafrost-table-id, then your project should be M060-).\nProject numbers are followed by the last name of the project lead.\nThen all projects should have a 2-4 word descriptive title (note - spend at least a bit of time thinking about this as this is what will be used on all other platforms (your personal computer, GitHub, GoogleDrive, etc…) as your project name.\n\nYour final project name is a combination of components 2 to 5 as follows: Lets say I wanted to create a project structure for the “Techniques for Field Identification of the Permafrost Table” manuscript I am working on, and lets say it is the 59th project in the registry.\nMy project name is then TypeNumber-Word1-Word2-Word3. So, the project name for my manuscript would be:\nM059-jelinski-permafrost-table-ID\n\n\n1.2.2.2 Naming Proposals\nAll lab-wide proposals (following project setup) will eventually have a GitHub repo which is built from the jelinski-lab-pedology-MXXX-project-template repo, and therefore coordinated and standardized naming is essential. Here is a style guide for naming your projects:\nProposals should be designated PL or PI (for proposal lab or proposal individual (see below)) and numbered by year. If the proposal is awarded then the entire proposal folder is copied to a grants folder in Areas (see below) with the same number. Group-wide grants (i.e. that Nic writes) start with the big PL label. Individual grants (student grants or personal applications) get a PI label so they do not conflict. Basically Nic is the only one who creates group proposal projects that are registered on the (jelinski-lab-project-registry?). Your proposals (which should start with PI) can have your own numbers, starting with 01).\nProposal names begin with PL or PI, then add the two digit year, and a unique two digit proposal ID. For numbers less than 10, pad to the left with a zero. Then, add your name, the name of the funder, and a short two to four word descriptive title.\nExamples: - PL23-05-jelinski-maelc-soil-texture - PI23-01-ainuddin-cogs-travel-grant\n\n\n1.2.2.3 Naming Conference Presentations and Lectures\nConference presentations and invited lectures do not need to be named and coordinated across all lab members, so ID numbers do not need to be unique. Many conference presentations may eventually end up in manuscript repos if a published manuscript presents the data.\nConference presentations or invited lectures begin with C or L, followed by the two digit year, person giving the presentation, conference or meeting name, and two to four word descriptive name.\nExamples: - C22-03-jelinski-sssa-cryoturbation-review - L23-01-jelinski-waterresources-soil-landscape-health"
  },
  {
    "objectID": "02a-para-system.html#what-is-an-area-how-should-i-name-my-areas",
    "href": "02a-para-system.html#what-is-an-area-how-should-i-name-my-areas",
    "title": "1  Overview of the PARA system",
    "section": "1.3 What is an Area? How should I name my Areas?",
    "text": "1.3 What is an Area? How should I name my Areas?\n\n1.3.1 Defining Areas - How to Determine if Something is an Area\nAreas are larger and more nebulous to define than projects, and they are also often recurring and don’t necessarily have an endpoint (although they might have milestones or deliverables nested within them or an end date). The following are examples of Areas:\n\nTeaching a course or invited lectures.\nGrants (including grant reports). Although grants do have a definable end date, they should still be areas because they are highly collaborative and typically have multiple projects under them. Grants typically end with a whimper and not a bang so are best defined as areas. Projects end with a bang!\nService\nEmployment related items such as annual reviews or summaries\n\n\n\n1.3.2 Naming Areas\nArea names can be a bit more general than project names and (with the exception of grants) they are more personal so do not need to be registered or have a GitHub repository (although they certainly can if it makes sense).\n\n1.3.2.1 Naming Grants\nAll lab-wide proposals (following project setup) which are funded become grants. The contents of the proposal folder are copied into Areas and the name remains the same except the PL or PI is removed and replaced with G. Grants should also eventually have a GitHub repo which is built from the jelinski-lab-pedology-MXXX-project-template repo, and therefore coordinated and standardized naming is essential. Here is a style guide for naming your projects: NOTE - need to create project folder/repo template for grants\n\n\n1.3.2.2 Naming Teaching Folders/Repositories\nTeaching folder should start with a T and then the two digit year followed by the course number and semester. Teaching folders may or may not have an associated GitHub repo - they should definitely have a Google Drive folder/archive of the same name. NOTE - need to create project folder template for teaching?\nExamples: - T23-soil2125-spring\n\n\n1.3.2.3 Naming Service Folders/Repositories\nService obligations are personal and therefore do not need to be coordinated across lab members. As service obligations can stretch across many years, these are not labelled by year - simply sequentially.\nExamples: - S07-cfans-uprc"
  },
  {
    "objectID": "02a-para-system.html#what-is-a-resource-how-should-i-name-my-resources",
    "href": "02a-para-system.html#what-is-a-resource-how-should-i-name-my-resources",
    "title": "1  Overview of the PARA system",
    "section": "1.4 What is a Resource? How should I name my Resources?",
    "text": "1.4 What is a Resource? How should I name my Resources?\n\n1.4.1 Defining Resources - How to Determine if Something is a Resource\nResources are things you actively, continuously refer to but that fall outside of projects and areas. Resources examples are:\n\nA note that contains all accounting codes\nA folder of templates for email responses to common inquiries\nAn actively maintained or growing list of books to read\nYour CV\n\n\n\n1.4.2 Naming Resources\nResource names can also be quite general and can also be numbered. They should start with “R”. An example would be “R01-accounting”, which could include a note that had all of the EFS strings you use, etc. Resource folder should be best thought of as tags perhaps? NOTE: how to deal with shared resources on Google Drive or GitHub? Need to have a resource registry to parallel the project registry? YES"
  },
  {
    "objectID": "02a-para-system.html#what-is-the-archive",
    "href": "02a-para-system.html#what-is-the-archive",
    "title": "1  Overview of the PARA system",
    "section": "1.5 What is the Archive?",
    "text": "1.5 What is the Archive?\nThe archive is for anything that doesn’t fit into the first three categories. This could be individual files such as unmaintained or inactive lists, or completed projects and areas that are no longer relevant to the work you are doing. Whole folders can be moved from the previous three categories into Archive."
  },
  {
    "objectID": "02a-para-system.html#example-folder-structure",
    "href": "02a-para-system.html#example-folder-structure",
    "title": "1  Overview of the PARA system",
    "section": "1.6 Example folder structure",
    "text": "1.6 Example folder structure\nHere is an example folder structure on my computer\n\n./Documents\n\n/00-projects\n\n/01-manuscripts\n\n/M003-jelinski-nayabeda-P\n/M004-jelinski-pb-distributions\n/M005-jelinski-platy-e-horizons\n/M020-jelinski-cold-soils-chapter\n/M059-jelinski-permafrost-table-id\n\n/02-proposals\n\n/PL23-01-jelinski-nrcs-urban-soil-mapping\n/PL23-03-jelinski-transtlantic-permafrost-thaw\n\n/03-conferences\n\n/C22-04-jelinski-sssa-cryoturbation-review\n\n\n/01-areas\n\n/01-grants\n\n/G23-03-jelinski-nrcs-urban-soil-mapping\n/G23-08-jelinski-maelc-soil-texture\n\n/02-teaching\n\n/T23-soil2125-spring\n/T23-laas5101-spring\n\n/03-personnel\n\n/undergraduate-students\n\njones-ben\n\n/graduate-students\n\nlohese-al\n\n/staff\n\nlabine-kat\n\n\n/04-service\n\n/S12-editing-ppp\n/S23-cfans-uprc\n\n/05-general-accounting\n\n/02-resources\n\n/R01-personal\n/R02-literature-notes\n/R03-email-responses\n\n/03-archive\n\n/M001-jelinski-crb-gelisols\n/M002-jelinski-us-eroded-soils"
  },
  {
    "objectID": "02a-para-system.html#why-do-this",
    "href": "02a-para-system.html#why-do-this",
    "title": "1  Overview of the PARA system",
    "section": "1.7 Why do this?",
    "text": "1.7 Why do this?\nAt this point you might be thinking this seems overly complicated. Why should I bother doing this, and why set up an unneccessarily strange and “computer” looking folder structure when I can just have folders named like “My Soil Project 2018”? I know what that is and I only have one project right now anyway…\nIf you are early in your career, this may seem unneccessary. But I can promise you this is actually the perfect time to start. This folder structure allows you to do five things:\n\ncreate a publicly available repository out of any one of your project directories at anytime with little to no work\nshare your folder with anyone in our lab group (or anyone who reads this workflow document) who with little further direction will be able to understand your files and where to locate things\nmaintain an increasingly complex and growing list of projects and areas as you move forward in your career with little to no headache\nspend little to no time finding exactly what you are looking for. As you grow this system, the numbers associated with particular projects will become secondhand nature - “oh that manuscript is M007”.\nas you complete projects or certain areas are no longer relevant to your job, you can move whole folders into the archive without compromising your ability to find things. Note, in the example above, how two manuscript folders (M01 and M02) have been moved to the archive. They retain the numbers I associated with them when I was working on them."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#registering-a-unique-project-id-on-jelinski-lab-project-registry",
    "href": "02b-project-subfolder-structure.html#registering-a-unique-project-id-on-jelinski-lab-project-registry",
    "title": "2  Project Subfolder Structure",
    "section": "2.1 Registering a unique project ID on (jelinski-lab-project-registry?)",
    "text": "2.1 Registering a unique project ID on (jelinski-lab-project-registry?)\n**If you are in my lab group*, when using this template to set up a new project repo, please first obtain a new project ID by adding your project as a new row to (jelinski-lab-project-registry?). NOTE: If you are not in my lab group, feel free to use this template repo and name it whatever the heck you want! Simply add a row with the next number in sequence (i.e. if the last registered project is M059-jelinski-permafrost-table-id, then your project should be M060-xxxxx) followed by your last name (assuming you are the project lead/owner) and then a 2-4 word descriptive title for your project (note - spend at least a bit of time thinking about this as this is what will be used on all other platforms (your personal computer, GitHub, GoogleDrive, etc…) as your project name."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#cloning-the-template-to-your-personal-github-page",
    "href": "02b-project-subfolder-structure.html#cloning-the-template-to-your-personal-github-page",
    "title": "2  Project Subfolder Structure",
    "section": "2.2 Cloning the template to your personal GitHub page",
    "text": "2.2 Cloning the template to your personal GitHub page\nAssuming that you are the project lead/owner, the first step is to clone this template to your personal GitHub page and set it as private initially. Any manuscripts/proposals in development (i.e. not published or awarded) should be set as private by default. Once manuscripts are in press or published then you can flip the repo to public."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#rename-the-template-repo",
    "href": "02b-project-subfolder-structure.html#rename-the-template-repo",
    "title": "2  Project Subfolder Structure",
    "section": "2.3 Rename the template repo",
    "text": "2.3 Rename the template repo\nNow, rename the template repo using your registered project ID."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#default-repo-directory-and-subdirectory-structure",
    "href": "02b-project-subfolder-structure.html#default-repo-directory-and-subdirectory-structure",
    "title": "2  Project Subfolder Structure",
    "section": "2.4 Default repo directory and subdirectory structure",
    "text": "2.4 Default repo directory and subdirectory structure\nUse the default standardized folder structure unless there is an explicit reason to do something different!\nThis standardized structure is my own, modified and inspired from many excellent sources 123456. This is likely a living template and will change/improve as we adapt to diverse projects. Here is an example of the template repo structure for an arbitrary project [M009-jelinski-permafrost-id].\n\n./M009-jelinski-permafrost-table-id\n\n/00-data\n\n/a-raw\n/b-prepared\n\n/01-code\n\n/a-Rscripts\n/b-markdowns\n\n/02-output\n\n/a-figures\n/b-tables\n\n/03-support-files\n\n/archive\n/cache\n/logs\n/temp\n\n/04-manuscript\n\n/a-drafts\n/b-submitted\n/c-final-and-proofs\n\n/05-metadata\n/06-presentations\n/docs\n[LICENSE]\n[MXXX-project-name.Rproj] will initiate in section X\n[README.md]\n\n\nNote that what I dodn’t use here was -lib or -libs and -tests"
  },
  {
    "objectID": "02b-project-subfolder-structure.html#what-do-each-of-these-folders-mean-and-what-should-i-put-in-them",
    "href": "02b-project-subfolder-structure.html#what-do-each-of-these-folders-mean-and-what-should-i-put-in-them",
    "title": "2  Project Subfolder Structure",
    "section": "2.5 What Do Each of These Folders Mean and What Should I Put in Them?",
    "text": "2.5 What Do Each of These Folders Mean and What Should I Put in Them?\nAlthough this list may seem daunting at first, each of these folders has a purpose, and this structure allows for deep integration with transparent data curation and data analysis workflows using R. Some of these folders may remain empty for the duration of your project and that is just fine! Note that each of the folders have a number convention for quick association once you start using this standardized structure. The numbers also allow for the folders to be arranged in a logical order, rather than alphabetically.\n\n2.5.1 /00-data\nThis folder contains all of the input data being used in this project. EVERY dataset (whether external or project generated) should have an associated metadata file! For complex projects with many datasources, each dataset should live in its own subfolder.\n\n2.5.1.1 /00-data/a-raw\nThe raw data: unmodified, comprehensive, containing outliers, missing values, imperfections and other items that may be removed in data pre-processing (sometimes called “munging”78). Insert link to data pre-processing chapter. It also contains any foundational geospatial data from other authors/sources that you did not generate but that you are pulling in for the purposes of analysis and that needs further processing to be utilized in your workflow. For example if an external data source came in a proprietary format that needed to be exported and modified. Note - need to create a subfolder separate for this, and also need to have a way to track origin and authorship - probably a readme or metadata file .txt or something - can be a running open file that is added to - maybe a markdown or something?\nAny raw data that you generated should be in open and easily readable formats. Raw data from other sources may not be in these open and readable formats and require further processing or preparation to make usable Specifically:\n\nTabular data should be in .csv format\nSpatial vector data (points, lines or polygons) should be in OGC geopackage format.\n\nAny data in raw data that is not your own should be in an /external-sources subdirectory which contains a metadata file that details the source of each of the external data sources.\n\n\n2.5.1.2 /00-data/b-prepared\nThis folder is the sink where all of your post-processed data will live. It also contains any foundational geospatial data from other authors/sources that you did not generate but that you are pulling in for the purposes of analysis following any necessary processing to change formats - note that this can include reprojected data, etc? actually maybye it shouldn’t because all of that can go into an R script - hmm need to think about this. Note - need to create a subfolder separate for this, and also need to have a way to track origin and authorship - probably a readme or metadata file .txt or something - can be a running open file that is added to - maybe a markdown or something? Data should be in open and easily readable formats. Specifically:\n\nTabular data should be in .csv format\nSpatial vector data (points, lines or polygons) should be in OGC geopackage format.\nSpatial raster data should be in GeoTIFF format.\n\n\n\n\n2.5.2 /01-code\nThis folder contains all of your source code and R scripts that you use to conduct your data processing and analysis as well as markdowns which describe your processing pipeline in depth (if you choose).\n\n2.5.2.1 /01code/a-Rscripts\nThis folder contains all of your R scripts. Note link to how to actually write and breakdown and organize scripts and analysis in projects - this can be in your R style guide or as a separate section - talk about sequential numbering, a master script, functions script, global script, readme, and how to renumber or keep your growing scripts squared away.\n\n\n2.5.2.2 /01code/b-markdowns\nThe idea for this folder comes from the pipeline subfolder idea from Ties de Kok9. Ties suggests that every script file in 01-code has a subfolder in a 02-pipeline subflder. Within each subfolder are three other directories: tmp, store, and out. Per Ties10:\n\n02_pipeline/sub-folder/out contains files that you save with the intention of loading them in a future code file. These are usually the “end-products” of the current code file.\n\n\n02_pipeline/sub-folder/store contains files that you save with the intention of loading them in the current code file. This is, for example, used in scenarios where it takes a while to run parts of your code and to avoid having to re-run these parts every time you might want to intermittently save the progress of your generated data to the store folder.\n\n\n02_pipeline/sub-folder/tmp contains files that you save for inspection purposes or some other temporary reason. The basic principle is that you should not have to worry about anything in the tmp folder being deleted as it only serves a temporary purpose.\n\nI really like this idea, however in reflecting think that a similar thing can be accomplished with markdown files and/or a quarto ebook. I want to keep the script folder clean and just R scripts (don’t want to junk it up with markdowns), so I actually think the best solution is to have a separate folder for markdowns. Note that these markdowns can then be used as inputs to build an ebook with Quarto and display as a GitHub Pages website.\nSo, this folder is to store markdowns from the individual scripts, or from groups of scripts. NOTE: NEVER EVER copy and paste code from R scripts to markdowns - the potential for error is just too high. Instead, call R scripts within the markdowns1112.\n\n\n\n2.5.3 /02-output\nThis folder contains final, polished figures and tables or other products that will go in the manuscript. Note - although the /04-manuscript folder has subfolders for tables and figures, these are not necessarily the same thing. Any figures and tables in /02-output are generated from your code and source data. It is likely/possible that you may also end up with manuscript figures and tables that are not generated from your source data (conceptual figures, logic models, etc).\n\n2.5.3.1 /02-output/a-figures\nFigures generated from data-code workflow in R\n\n\n2.5.3.2 /02-output/b-tables\nTables generated from data-code workflow in R\n\n\n\n2.5.4 /03-support-files\nThe support files folder contains information that may help support understanding of your project development, workflow, and speed up processing through the use of a cache. However these files should NEVER be strictly necessary for reproducing your data, analysis, and outputs.\n\n2.5.4.1 /03-support-files/archive\nThis folder contains anything that is not relevant to the current workflow but that you are not ready to delete permanently yet. Items can be moved to the archive folder at any time. Consider including in git-ignore.\n\n\n2.5.4.2 /03-support-files/cache\nYour cache folder may not be used often. However it is there to provide a storage place for any large objects or data that take a very long time to generate initially in a script. When these are generated once, they can be exported to this folder - this provides a shortcut for subsequent running of the scripts that saves time. People can still run your original scripts to generate these objects if they so choose.\n\n\n2.5.4.3 /03-support-files/logs\nThis folder contains logs related to your project, specifically your analysis log and writing log. You can choose to make these public or not. See this section for how to write and construct logs - do it at the end of every day/session - take the 5 MINUTES!!!\n\n\n2.5.4.4 /03-support-files/temp\nThe temp folder is basically to hold any temporary non-important files, such as tests script outputs, test figures, etc. These are not important to the overall analysis. This folder should be not be included in your final project using [.gitignore] file.\n\n\n\n2.5.5 /04-manuscript\nThis folder houses everything related to manuscripts or reports resulting from the project. It has subfolders for holding drafts, submitted versions (inclduign subsequent revisions), final and proofs, final figures & tables, and references (which contains a single .bib file - ideally exported from your Zotero project folder which matches the name of this project).\n\n2.5.5.1 /04-manuscript/a-drafts\nManuscript drafts. If there are critical draft versions (like those sent to collaborators at certain times, those should get a subfolder with date of draft - i.e. 10JAN2023). You do not need to save regular working drafts with specific dates unless you choose. There should be further figures, tables, and refs subfolders within those containing the versions of figures and tables used, and a .bib file in the refs folder.\n\n\n2.5.5.2 /04-manuscript/b-submitted\nSubmitted versions of manuscripts should be placed in a subfolder in here with date of submission - i.e. 10JAN2023). There should be further figures, tables, and refs subfolders within those containing the versions of figures and tables used, and a .bib file in the refs folder.\n\n\n2.5.5.3 /04-manuscript/c-final-and-proofs\nFinal accepted version of manuscript and galley proofs."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#metadata",
    "href": "02b-project-subfolder-structure.html#metadata",
    "title": "2  Project Subfolder Structure",
    "section": "2.6 /05-metadata",
    "text": "2.6 /05-metadata\nThis folder contains all metadata files and metadata products. - MXXX-project-name-metadata.json: This is your metadata file for the entire project. This metadata file should be formatted according to our general conventions on metadata here. - MXXX-project-name-metadata.html: This is the .html file generated from dataspice -/csvs: this is a subdirectory to contain foundational .csv files used to build the .json and html using the dataspice package 13.\nThis is your metadata file for the entire project (note that all datasets should also have associated metadata files as well in the 00-data directory. This metadata file should be formatted according to our general conventions on metadata here.\n\n2.6.1 /06-presentations\nThis folder should contain subfolders for each presentation realted to the topic. Subfolders should start with “C0X” and also include the year and venue. So if I gave a presentation at the Soil Science Sciety of America meetings in 2017 on a particular project, and it was the first presentation that I gave under that project, my subdirectory containing all of the relevant things for that presentation would be “C01-2017-SSSA”.\n\n\n2.6.2 /docs\nIf you choose to create a Quarto e-book to consolidate your metadata and readmes as well as project workflow and potentially manuscript, all relevant files should go in this subdirectory. More information on generating an html book from separate markdown files here. NOTE - the only way to get GitHub Pages to find the files and render the htmls together is to have them in a /docs subdirectory. There is no other options currently. That is why /docs doesn’t have a number before it.\n\n\n2.6.3 LICENSE\nThis is a standard open source license shipped with this template. I am choosing to use the MIT license currently, but this may not be the best choice1415.\n\n\n2.6.4 MXXX-project-name.Rproj\n**NOTE: even though this contains a .Rproj file, you should always use the here::here function in the here package in R when referencing file locations in your project repo - there are excellent reasons for using both .Rproj and here::here - when combined these are a powerful way to guarantee there won’t be any issues with other users running your code due to directory issues. Malcom Barrett gave an an excellent talk at the useR! 2020 conference entitled “Why use the here package when you’re already using projects?”. The .Rproj file contained in this tenplate is pre-configured to start with a completely clean workspace EVERY TIME by selecting in Project Options &gt; General “Restore .RData into workspace at startup” = NO and “Save workspace to .RData on exit” = NO, “Disable .Pprofile execution on session start/resume” = CHECKED, “Quit child processes on exit” = CHECKED. These two options combined will also ensure that others running your code (or even you at a later date) don’t experience errors or conflictions due to workspace-specific background16 - this is often the cause of the following issue: “I swear…my code worked the last time I ran it and I didn’t change anything!”.\n\n\n2.6.5 README.md\nThis is arguably one of the most important features of your entire project. The readme should always be in plain text .txt format so that anyone on any system can read it. Spending time on your readme file is extremely important:\n\nIt is the first writing you will do on your project and helps you to describe at a high level what your project is about\nIt is the file that tells everyone the who, what, when where, why, and how of this project:\n\nwho created this project?\nwhat is in this project?\nwhen was it created?\nwhere can i find relevant data, files, and documents within the project structure?\nwhy was this project done; what was the motivation?\nhow should I use these project files; what are the file formats, what programs do I need to run or access them; how were these files created and using what tools?\n\n\nA readme is a living document. You should begin any project with a draft readme, which, provided you use this workflow and file structure can be mostly copied from a suggested template located in this section. However, as your project grows and changes, your readme should be updated. Be as thorough as possible. Note that this is a project level readme. Although not required, you should strongly consider including sub readmes at within multipl subfolders where necessary. Conversely, you can create a project level book in Quarto that contains all readmes and documentation (see section X). More information on how to use and write a readme in section X.\n\n2.6.5.1 A note on a TODO list. Do not keep a list of TODOs in a separate file. Instead log them as issues on GitHub. More powerful, more flexible.\nThis is a running list of TODOs - actually this shouldn’t be a thing. You should use GitHub Issues to track this? Not sure need to look into this more."
  },
  {
    "objectID": "02b-project-subfolder-structure.html#footnotes",
    "href": "02b-project-subfolder-structure.html#footnotes",
    "title": "2  Project Subfolder Structure",
    "section": "",
    "text": "Alex Douglas::Setting up a reproducible project in R↩︎\nTies de Kok::How to keep your research projects organized: folder structure↩︎\nKenyon White::ProjectTemplate↩︎\nProject Template↩︎\nAnna Krystalli::Projects in R Studio↩︎\nMatt Dray & Anna De Palma - rostrum.blog::A GitHub repo template for R analysis↩︎\nUSGS::Beyond Basic R - Data Munging↩︎\nTJ Murphy::Reproducible Data Munging in R↩︎\nTies de Kok::How to keep your research projects organized: folder structure↩︎\nTies de Kok::How to keep your research projects organized: folder structure↩︎\nSteve Shu::What is the best workflow for a new R user?↩︎\nThaufas::Using Code Chunks in R Markdown↩︎\nAnna Krystalli::Dataspice Tutorial↩︎\nGitHub Docs::Licensing a Repository↩︎\nchoosealicense.com::Choose an Open Source License↩︎\nAlex Douglas::Setting up a reproducible project in R↩︎"
  },
  {
    "objectID": "02c-writing-readmes.html#readme.md-and-project-metadata",
    "href": "02c-writing-readmes.html#readme.md-and-project-metadata",
    "title": "3  Writing Readmes - Your First Assignment",
    "section": "3.1 readme.md and Project Metadata",
    "text": "3.1 readme.md and Project Metadata\nWriting in your project actually begins with adding content to the readme.md file. This is the human-readable part of your project metadata, and should be as descriptive and narrative as possible.\nThe readme is arguably one of the most important features of your entire project. It forms a cornerstone of the metadata catalog for the entire project see metadata section #todo and what is metadata #todo and metadata components #todo. The readme should always be in plain text .txt or .md format so that anyone on any system can read it and it is as futureproof as possible. Spending time on your readme file is extremely important:\n\nIt is the first writing you will do on your project and helps you to describe at a high level what your project is about\nIt is the file that tells everyone the who, what, when where, why, and how of this project:\n\nwho created this project?\nwhat is in this project?\nwhen was it created?\nwhere can i find relevant data, files, and documents within the project structure?\nwhy was this project done; what was the motivation?\nhow should I use these project files; what are the file formats, what programs do I need to run or access them; how were these files created and using what tools?\n\n\nA readme is a living document. You should begin any project with a draft readme, which, provided you use this workflow and file structure can be mostly copied from a suggested template located in this section. However, as your project grows and changes, your readme should be updated. Be as thorough as possible. Note that this is a project level readme. Although not required, you can consider including sub readmes within multiple subfolders where necessary. Conversely, you can create a project level book in Quarto that contains all readmes and documentation (see section X). More information on how to use and write a readme in section X."
  },
  {
    "objectID": "02c-writing-readmes.html#suggested-template",
    "href": "02c-writing-readmes.html#suggested-template",
    "title": "3  Writing Readmes - Your First Assignment",
    "section": "3.2 Suggested Template",
    "text": "3.2 Suggested Template\nThe following text is an example readme template to populate the readme for your project when you first initialize the directory. It is also contained in the file in the /Resources folder of this repo called readme-template.txt. It is adapted from Cornell University Research Data Management Service Group.\n\nThis readme file was generated on [YYYY-MM-DD] by [NAME]  &lt;[text in square brackets should be changed for your specific dataset]&gt;\nGENERAL INFORMATION\nTitle of Dataset:\n Author/Principal Investigator Information Name: ORCID: Institution: Address: Email:\nAuthor/Associate or Co-investigator Information Name: ORCID: Institution: Address: Email:\nAuthor/Alternate Contact Information Name: ORCID: Institution: Address: Email:\nDates of data collection: &lt;provide single date, range, or approximate date; suggested format YYYY-MM-DD&gt;\nGeographic location of data collection: &lt;provide latitude, longiude bounding box, or city/region, State, Country&gt;\nFunding sources: &lt;include funding source and grant or agreemnet number, title and date range if applicable&gt;.\nSHARING/ACCESS INFORMATION\nLicenses/restrictions placed on the data:\nLinks to publications that cite or use the data:\nLinks to other publicly accessible locations of the data:\nLinks/relationships to ancillary data sets:\nWas data derived from another source? If yes, list source(s):\nRecommended citation for this dataset:\nDATA & FILE OVERVIEW\nFile List: &lt;list all files (or folders, as appropriate for dataset organization) contained in the dataset, with a brief description, organize this by subfolder&gt;\nNOTE: You may not see every subfolder listed here. By default, Git does not push subfolders that do not contain at least one file to the repository. Therefore, if you do not see one of the below listed folders, assume that there were no files in the folder at the time of the push.\n\n/00-data-raw this folder contains the curated raw data: unmodified, comprehensive, containing outliers, missing values, imperfections and other items that may be removed in data pre-processing. It also contains any foundational geospatial data from other authors or sources that were not generated as part of this project but were used in data analysis. Source information for external data provided below &lt;- list files&gt;\n/01-data This folder contains all post-processed data used for analysis. It also contains foundational geospatial data from other authors/sources following any necessary processing to change formats &lt;- list files&gt;\n/02-src This folder contains all of your source code and R scripts that were used to conduct data processing and analysis, and includes a .Rproj file to initialize the environment prior to running the scripts. &lt;- list files&gt;\n/03-cache This folder contains any large files generated as part of data analysis, which can be optionally used from this directory to save time running scripts. &lt;- list files&gt;\n/04-temp This folder is left purposely empty - it was used during analysis as a repository for temporary or experimental files.\n/05-manuscript This folder holds everything related to manuscripts or reports resulting from the project. It has subfolders for holding drafts, submitted versions (inclduign subsequent revisions), final and proofs, figures, tables, and references (contained in a single .bib file). &lt;- list files&gt;\n/06-presentations This folder contains a subfolder for each presentation given on this project. &lt;- list files&gt;\n/07-log This folder contains data analysis and writing logs &lt;- list files&gt;\n/08-archive This folder may be left purposely empty. If it is not empty, it contains anything that is not relevant to the current workflow but that was not permanently deleted; may not be well organized\n/09-docs This folder contains all files for generating a published, project level book in html or pdf form. This is hosted on GitHub Pages #NOTE this should be generated in both html and pdf form to ensure maximum readability. See section X #todo for how to build and generate a book. NOTE ALSO - the name of this cannot be changed if you want to publish an html through GitHub pages. It needs to render from docs. &lt;- list files&gt;\n[readme.txt] YOU ARE READING THIS NOW!\n[TODOs.txt]\n\nRelationship between files, if important:\nAdditional related data collected that was not included in the current data package:\nAre there multiple versions of the dataset? If yes, name of file(s) that was updated: Why was the file updated? When was the file updated?\nMETHODOLOGICAL INFORMATION\nDescription of methods used for collection/generation of data:  NOTE that a simple way to to this is to provide a link to our laboratory protocols or field protocols book, or to also put independent copies of them in the repo and/or just include that in your own project Quarto book #todo\nMethods for processing the data: \nInstrument- or software-specific information needed to interpret the data: &lt;include full name and version of software, and any necessary packages or libraries needed to run scripts&gt;\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:\nDATA-SPECIFIC INFORMATION FOR: [FILENAME] &lt;repeat this section for each dataset in the 00-raw-data and 01-data folders, folder or file, as appropriate&gt;\nNumber of variables:\nNumber of cases/rows:\nVariable List: &lt;list variable name(s), description(s), unit(s) and value labels as appropriate for each&gt;\nMissing data codes: \nSpecialized formats or other abbreviations used:"
  },
  {
    "objectID": "02c-writing-readmes.html#references",
    "href": "02c-writing-readmes.html#references",
    "title": "3  Writing Readmes - Your First Assignment",
    "section": "3.3 References",
    "text": "3.3 References\nCornell University Research Data Management Service Group: Guide to writing “readme” style metadata."
  },
  {
    "objectID": "02d-github-setup-workflow.html#getting-set-up-in-github-and-github-desktop",
    "href": "02d-github-setup-workflow.html#getting-set-up-in-github-and-github-desktop",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.1 Getting set up in GitHub and GitHub Desktop",
    "text": "4.1 Getting set up in GitHub and GitHub Desktop\nThis chapter assumes that you already have setup your personal GitHub account (either on public GitHub or on the University’s GitHub Enterprise instance; or both), have downloaded GitHub Desktop and linked or accessed your account and a test repository through the GitHub Desktop app, and understand some basics regarding GitHub functionality. If you have not yet done these things or need resources to help you get set up, check out the [GitHub/GitHub Desktop] chapter in the Tools section of this book."
  },
  {
    "objectID": "02d-github-setup-workflow.html#what-is-the-difference-between-github.com-and-github.umn.edu-and-where-should-my-project-go",
    "href": "02d-github-setup-workflow.html#what-is-the-difference-between-github.com-and-github.umn.edu-and-where-should-my-project-go",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.2 What is the difference between github.com and github.umn.edu and where should my project go?",
    "text": "4.2 What is the difference between github.com and github.umn.edu and where should my project go?\nFirst and most importantly, the University Enterprise version of GitHub github.umn.edu is a private instance of GitHub that is accessible only to students, faculty and staff (i.e. you must have a UMN x500 number to access it). Apart from the standard GitHub interface, look and feel, it is on completely different servers and does not talk to public GitHub. If you no longer have an x500, you will not be able to access UMN Enterprise GitHub. Because you may not be at the University of Minnesota for your entire career, you should generally default to setting up and using your personal profile on public GitHub, which you will be able to maintain and access in perpetuity. However, there are some limited instances when the UMN Enterprise version of GitHub might be preferred:\n\nYou want to create a repository that is only accessible to UMN students, faculty and staff.\nYou want to publish a Quarto book using GitHub Pages for a class, but only want students who are in the class at UMN to be able to see and access it. NOTE: generally speaking, I am of the philosophy that most general class materials such as the syllabus, lab manuals, and other exercises should be public, however there are some class items such as student submissions and essays that you would not want to be public. However for most classes, the use for GitHub would just be to publish a stable link to a Quarto book using GitHub Pages and point to it in your Canvas course site, since Canvas is the platform of choice for classes. Everything else regarding class management can and should be done in Canvas with the exception of some computer science classes that are writing and submitting code on GitHub as a matter of professional practice. UMN Enterprise GitHub instance makes a lot of sense for those classes but not for many others.\nYou want to “show your work as you go” on a project in this workflow, by utilizing a project book (modeled on Hava’s procedure), but do not want others to see this until you are ready to share it. On github.umn.edu, you can create a private repository and publish a Quarto rendered book using GitHub pages that only you will be able to see or that you will be able to tighly control access on. NOTE: On public GitHub, you have the ability to set repo access to private. However, when using the free tier of GitHub, you cannot publish a Quarto book to GitHub Pages unless your repo is public. Therefore, in order to keep a repo private and keep a book-based project and analysis log, everyone will be able to see your progress. Maybe this is ok, maybe it isn’t it is up to you and the nature of your project. Conversely - the “Team” tier for GitHub is (as of 2022) $44/yr, and allows you to publish private GitHub pages from private repos where you control access.\n\nBottom line: if you are a graduate student working on a project as part of your thesis, you can initiate your repo on UMN Enterprise GitHub if you choose (that way you won’t have to worry about privacy and can publish to GitHub Pages without paying). Especially for your first repo and when you are learning to use GitHub, this will also allow you to experiment with Git functionality without fear of public access in any way, shape, or form. However, just be aware then when your project moves to final stages you will need to create a new repository on your public GitHub and push from the repo on your local computer - you cannot simply mirror your UMN Enterprise repo to public GitHub. Not a big deal either way, but you should plan ahead for what is most appropriate for your project."
  },
  {
    "objectID": "02d-github-setup-workflow.html#what-is-the-difference-between-github-and-github-desktop",
    "href": "02d-github-setup-workflow.html#what-is-the-difference-between-github-and-github-desktop",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.3 What is the difference between GitHub and GitHub Desktop?",
    "text": "4.3 What is the difference between GitHub and GitHub Desktop?\nGitHub Desktop is a user interface which makes it easy to interact with repositories in your personal GitHub account and any organizations that you either created or are a member of. I highly recommend when you are just starting to learn how to use GitHub, and even beyond. There are other options for interacting with GitHub and GitHub repos - such as command line tools and through R Studio, but until you know the basics and understand what is happening and when, using command line tools will likely be more confusing. Once you know what you are doing, it can sometimes be more convenient to interact with your repos through R Studio (seldom is the command line more convenient), but its really not that much more convenient. Bottom line: Set up and use GitHub Desktop unless you have a compelling reason not to!"
  },
  {
    "objectID": "02d-github-setup-workflow.html#when-should-i-create-a-github-repo-for-my-project",
    "href": "02d-github-setup-workflow.html#when-should-i-create-a-github-repo-for-my-project",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.4 When should I create a GitHub repo for my project?",
    "text": "4.4 When should I create a GitHub repo for my project?\nStart an empty project at any time! I regularly create empty folders for manuscripts I just have an idea for or want to write. Especially if a project is also a GitHub repo, at the very least it encourages you to write some metadata, and have a file structure for when things do come up. Also, the project feels more official and branded, which can sometimes be a kick in the pants to get moving, esp if public.\nHow to decide when to make a repo on GitHub - some options:\n\nAnytime!\nOnce you have some materials in your project folder\nOnce the project is well defined\n\nit’s common for dissertation chapters and non-data (i.e. review or big idea manuscripts (what I will call luxury manuscripts) to be nebulous and change). These are usually not very collaborative either. You can wait to make GitHub repos for these until they coalesce or become better defined. BUT you should still have a local folder structure for them."
  },
  {
    "objectID": "02d-github-setup-workflow.html#when-should-i-create-an-organization-vs-a-repository",
    "href": "02d-github-setup-workflow.html#when-should-i-create-an-organization-vs-a-repository",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.5 When should I create an Organization vs a Repository?",
    "text": "4.5 When should I create an Organization vs a Repository?\nHere is where we can achieve some excellent deep integration with the PARA system. Generally speaking, only Projects and Areas should get repositories. Projects get individual Repositories while Areas get Organizations with individual project repositories under them. Resources and Archive should, generally speaking remain on your local computer or be in the cloud in Google Drive."
  },
  {
    "objectID": "02d-github-setup-workflow.html#how-should-i-create-the-github-repo-for-my-project",
    "href": "02d-github-setup-workflow.html#how-should-i-create-the-github-repo-for-my-project",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.6 How should I create the GitHub repo for my project?",
    "text": "4.6 How should I create the GitHub repo for my project?\nThere are a few different ways to [create your GitHub repo]#todo - link to Tools chapter. However, since most of us begin work on a project on our local computer, I highly recommend you start by building a local directory and then create a [GitHub repo using GitHub Desktop]#todo - link to tools chapter."
  },
  {
    "objectID": "02d-github-setup-workflow.html#jelinski-lab-github-repo-standards",
    "href": "02d-github-setup-workflow.html#jelinski-lab-github-repo-standards",
    "title": "4  Getting Your Project on GitHub: Standards and Workflow",
    "section": "4.7 Jelinski Lab GitHub Repo Standards",
    "text": "4.7 Jelinski Lab GitHub Repo Standards\n\n4.7.1 Repo Initialization\nRegardless of how and where you choose to set up your project repo, you should follow these standards:\n\nYour repo should follow the standard project directory setup as outlined in ?sec-subfolderstructure.\nYour repo should be initialized with a readme.txt file, which at the very least contains the readme template with as much current information as you can currently provide for your project.\nYou should [choose a license]. The default license for projects in our lab is the MIT license.\n\n\n\n4.7.2 Jelinski Lab GitHub Workflow Follows GitHub Flow\nOur standard GitHub workflow follows the well documented GitHub flow process, which is a branch based workflow:\n\nIf you are the sole person working on a repository, you can get away with pushing and pulling from the /main branch.\nHowever, if you are working on your repository with others (or even by yourself, but want to ensure you don’t inadvertently change your main branch while making edits and changes), you should create a branch within the repo and give it a descriptive name. You can modify, push and pull from the branch, and allow others to see what you have done.\nEventually, once you are happy with your changes (if you wish) merge it with the main branch to update everything. GitHub has some excellent tools for merging and for controlling through “pull request merges](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/merging-a-pull-request).\nAfter a branch has been fully merged, it can be deleted and pruned without loss of information as deleting a branch does not delete the commit history.\n\n\n\n4.7.3 Guidelines for Pushing to Your Repo\nAdditionally, you should do the following when working with your GitHub repo:\n\nStrive to commit and push to your repo at the end of each day of work on a project or at small milestones in your work. Writing good commit messages (see below) will help you understand when to do that. Don’t wait long periods of time to push your local repo/directory to your GitHub repo! This is incremental work!\nEvery commit should contain a good commit message, and a detailed description where possible. General guidelines that we will follow whihc are adapted from Robert Painsi, (Bolaji Ayodeji, including example from Tim Pope Toni Bardina and GitKraken:\n\nCommit messages should have an entry in the subject and body\nSubject line should be capitalized and less than 50 characters\nSpaces are allowed\nNo period at end of subject line, punctuations allowed in body of message (description on GitHub Desktop)\nSubject line written in imperative mode (i.e. Add, not Added or Adds)\nSubject line should begin with one of the following verbs (which are adapted from their comp sci meanings to what best fits our work - see full list here):\n\nAdd - added a document, file, code, or section within a document\nDelete - added a document, file, code, or section within a document\nEdit - edited a document, file, code, or section within a document\nFix - fix an issue e.g. bug, typo, accident, misstatement.\nStart - begin doing something\nStop - end doing something\nRefactor - reorganize or refactor code\nRestructure - reorganize or refactor directory or project structure\nDocument - editing, changing or adding to metadata\n\n\nThe commit message body should only contain what and never why or how. Whys and hows should go into documentation such as project logs (writing or analysis logs), metadata, or the project Quarto book (if you have one)."
  },
  {
    "objectID": "02e-integrating-zotero.html#zotero-folder-structure-for-your-project",
    "href": "02e-integrating-zotero.html#zotero-folder-structure-for-your-project",
    "title": "5  Setting Up Zotero for Your Project - Early Success with Writing and Reference Management",
    "section": "5.1 Zotero Folder Structure for Your Project",
    "text": "5.1 Zotero Folder Structure for Your Project\nThere are a few things you should do to setup Zotero to deeply integrate with your project folder structure. Assuming you have at least one project you are working on, your Zotero library should, at a minimum, have the following Collections (i.e. main folders under the Library icon):\n\nA folder that you will call “000 Inbox”. This has three leading zeros to ensure this folder remains at the top of your library regardless of what you name the other folders.\nFolders that mirror your high-level PARA system#todo folders (i.e. “00-projects”, “01-areas”, “02-resources”,“03-archive”). You should then set up a folder (i.e. subcollection) under the 000-projects folder that is named the same as your project directory or repo name (i.e. “M009-jelinski-permafrost-table-id”). This ensures that you collect relevant references in a folder that has exactly the same name as your repo or directory. The key is association, number and name - across apps."
  },
  {
    "objectID": "02e-integrating-zotero.html#integrating-zotero-with-your-browser",
    "href": "02e-integrating-zotero.html#integrating-zotero-with-your-browser",
    "title": "5  Setting Up Zotero for Your Project - Early Success with Writing and Reference Management",
    "section": "5.2 Integrating Zotero With Your Browser",
    "text": "5.2 Integrating Zotero With Your Browser\nYou should install the Zotero Connector add-on to your browser of choice (Chrome or Safari). Ensure the add-on is enabled - it will show up as a small icon to the right of your address bar."
  },
  {
    "objectID": "02e-integrating-zotero.html#caching-relevant-references-as-you-go",
    "href": "02e-integrating-zotero.html#caching-relevant-references-as-you-go",
    "title": "5  Setting Up Zotero for Your Project - Early Success with Writing and Reference Management",
    "section": "5.3 Caching Relevant References As You Go",
    "text": "5.3 Caching Relevant References As You Go\nAs you begin your project and go about your other work, you will slowly start to come across relevant referencesor interesting references not relevant to your current project that you would like to keep. This deep integration makes it easy to cache and later find all of the references that you come across prior to the point where you are spending time on dedicated wirting on your manuscript. This allows you to begin building a bibliography before you even begin writing your manuscript in earnest. If you are searching for references online or come across a reference online, simply:\n\nMake sure Zotero is open\nEnsure your “000 Inbox” folder is highlighted\nClick the Zotero add-on icon in your browser to import the full paper reference (note: this also works for websites and books).\nIf you have access to the pdf (i.e. if it is open sources or if you are logged in to University internet or remotely through VPN to the University Libraries), the pdf will also be automatically downloaded and attached to your reference record.\n\n*NOTE: I prefer to send everything to the Inbox first and then later go through the inbox to shuffle records to the relevant folders. However, if you choose you could also highlight the relevant folder instead of 000 Inbox and Zotero will send the reference there instead of to your inbox. Note that Zotero allows you to place a single reference in two or more collections or subcollections without duplication of the original file. This is very handy. There are many other excellent Zotero features - not least of which is [Integration with Obsidian]#todo, merging duplicate references, text and area highlighting in PDFs, and sharing reference collections."
  },
  {
    "objectID": "03-pre-data-raw-data.html#project-types",
    "href": "03-pre-data-raw-data.html#project-types",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Project Types",
    "text": "Project Types\nProjects come in many different shapes and forms. For the purposes of this book, we will use four main categories of project which should fit most projects happening in our lab. Note that these project types refer to where the major portion of the raw data is coming from:\n\nField projects\nLaboratory projects\nReview projects\nData synthesis projects\n\nNote that all projects may contain some components of several of these categories, but generally speaking, there should be a single category that best fits your project and can be used as a template to set up your pre-data infrastructure. You may also combine tools from multiple categories as appropriate for your project."
  },
  {
    "objectID": "03-pre-data-raw-data.html#field-projects",
    "href": "03-pre-data-raw-data.html#field-projects",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Field Projects",
    "text": "Field Projects\nI’ll define field projects here as projects that require doing fieldwork to collect novel physical samples or morphological data. These projects require travel of some sort to collect raw data. Raw data for these projects typically will consist of physical samples."
  },
  {
    "objectID": "03-pre-data-raw-data.html#laboratory-projects",
    "href": "03-pre-data-raw-data.html#laboratory-projects",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Laboratory projects",
    "text": "Laboratory projects\nThe raw data for laboratory projects consists of laboratory analysis of existing soil samples or samples consolidated from collaborators."
  },
  {
    "objectID": "03-pre-data-raw-data.html#review-projects",
    "href": "03-pre-data-raw-data.html#review-projects",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Review Projects",
    "text": "Review Projects\nThe raw data for review projects is literature itself (peer-reviewed, gray or unpublished)."
  },
  {
    "objectID": "03-pre-data-raw-data.html#data-synthesis-projects",
    "href": "03-pre-data-raw-data.html#data-synthesis-projects",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Data Synthesis Projects",
    "text": "Data Synthesis Projects\nData synthesis projects consolidate published and unpublished data from other scientists and collaborators."
  },
  {
    "objectID": "03-pre-data-raw-data.html#additional-resources",
    "href": "03-pre-data-raw-data.html#additional-resources",
    "title": "“Pre-Data” and Raw Data: Setting Up and Executing Raw Data Collection and Curation Infrastructure for Different Project Types",
    "section": "Additional Resources",
    "text": "Additional Resources\nUSGS::Data Management - Quality by Design: Recommended Practices USGS::Data Management - Domain Management: Recommended Practices"
  },
  {
    "objectID": "03a-field-projects.html#pre-data-infrastructure",
    "href": "03a-field-projects.html#pre-data-infrastructure",
    "title": "6  Field Projects",
    "section": "6.1 Pre-Data Infrastructure",
    "text": "6.1 Pre-Data Infrastructure\n\n6.1.1 Survey123 for tabular data, spatial data, and image collection\nOur standardized tool for data collection for field projects is ArcGIS Survey123. Survey123 is a tool that allows you to create customized data entry and submission forms that you can then access through the Survey123 app on your phone. These forms can be created through the ArcGIS Online Survey123 portal. Although it is a proprietary tool, all faculty, students and staff at UMN have full, free access to the ArcGIS Online suite of tools which includes Survey123. Despite the fact that Survey123 is proprietary and not open source, it is highly functional and therefore the ease of use makes it our tool of choice. This chapter assumes you are familiar and have accessed Survey123. If you have not worked with Survey123 before, it is well supported and documented by ESRI and you can check out our Tools: Survey123 chapter #todo in this book.\n\n\n6.1.2 Lab Standards for Survey123 Form\nAs you set up your Survey123 form, you should think through each of the pieces of information that will be important to collect in the field and provide a way to capture that information in your form. When in doubt - err on the side of including it in your form - you can always ignore it in the field while using the app if there is some redundancy.\nIf you are doing a field project, you should strongly consider using Survey123 to collect your plot or site-level data. Think of what unit of your sampling will have a unqiue latitude and longitude and that is the data collection unit that your survey form should be designed for. For example, if you have a project that requires taking soil cores, with multiple samples from different depths taken from each core, then the survey form should be designed around each of the cores, which will have a unqiue latitude and longitude, and not each of the samples (many of which from the same core will have the same latitude and longitude).\nSurvey123 excels at consolidating spatial data (latitudes and longitudes), tabular data (text, numbers, or X entered in response to questions on the form), and images (taken in response to questions on the form) in one location. When you submit each of your records, this raw data will live in the Survey123 database through ArcGIS Online that is associated with your form.\nOur laboratory standards for Survey123 forms is that they should (at a minimum) collect the following information:\n\nName of initials of person recording data or submitting record\nGPS location of record (autocollected in response to location question)\nDate and time of record collection (autocollected in response to time question)\nUnique plot or observation number in the context of the project\n\nYour Survey123 form should also include fields to enter all relevant raw data as required by your project.\n\n\n6.1.3 Additional Survey123 Functionality of Interest\nImages. Survey123 is an excellent way to collect and organize images. Single image submission questions can be configured to collect multiple images. Once submitted, all images are stored with the associated record in the online database.\nDeep Customization - XLSForms Deeper functionality for setting up forms in Survey123 requires a PC and the use of XLSForms and Survey123 Conect (a desktop app that only runs on Windows). While this is very annoying for us Mac users, it allows extensive customization of your Survey123 form, including but certainly not limited to:\n\nRepeats: “Repeats” allow you to generate multiple records (rows) from a single survey entry/submission - so for example, you can generate multiple rows for the master sample log (each row is a sample) from one submission form (plot or pedon): https://community.esri.com/t5/arcgis-survey123-blog/survey123-tricks-of-the-trade-repeats/ba-p/898041\nCustomizing File Names for Submitted Images: Customaization through XLS Forms allows you to set the file names of submitted photos - this is really important - automatically annotate photo file names with project, plot, and question or anything else from designated fields in the form (as well as date/time). This functionality can save significant time when you go to curate your photo archive. https://community.esri.com/t5/arcgis-survey123-blog/survey123-tricks-of-the-trade-set-the-file-name-of/ba-p/1217125\n\n\n\n6.1.4 Hard Copy Field Data Sheets\nSurvey123 is an excellent tool for collecting plot or site level data in the field with your phone. However, some types of field data collection are extremely difficult to digitize under field conditions. Examples of these are:\n\nSoil morphology data (lots of specialized acronyms, notes, and chicken scratches add depth to the data, often difficult to use phone while textureing soil, many fields and extremely time consuming to use an app or tablet in the field).\nData consisting of many numbers or that exists at the sample level. Remember I said that Survey123 really shines when used at the level of unique geospatial coordinates which is typically the plot or site level. However, often times we may also be collecting data at the sample level. For example recording the dimensions of a permafrost soil core section that will go into a sample bag, or the field pH and EC of a soil sample. These data are best collected using hard copy forms. There are examples of hard copy forms for field data collection that Megan Andersen put together in the Jelinski Lab Field Handbook repository [here]#todo. Always print your hard copy forms on Rite-In-The-Rain!!"
  },
  {
    "objectID": "03a-field-projects.html#raw-data-collection",
    "href": "03a-field-projects.html#raw-data-collection",
    "title": "6  Field Projects",
    "section": "6.2 Raw Data Collection",
    "text": "6.2 Raw Data Collection\nAfter your pre-data infrastructure is set up, do some test runs and some mock data collection around campus. This will allow you to use your infrastructure, datasheets, and think through any missing pieces or functionality issues. Revise as needed and get ready for your travel - check out our [Field Manual]#todo for protocols and processes related to field work.\n\n6.2.1 Tips for using Survey 123 in the field\nSurvey 123 is generally an excellent app in the field, and most new phones are sufficiently durable and waterproof to be used in the field. As recently as 2019 I carried a separate digital camera and GPS. Now I do everything (capture GPS, take pictures, use Survey123 for tabular data) on my phone. *NOTE - this also means you should have a backup field ready rugged battery pack to charge your phone during the day if necessary in the field. There are also some other tips when using Survey123 in the field.\n\nImages. When choosing to use Survey123 as an image repository, ensure that you 1) take pictures with your phone and upload them to a survey response from your photo library (instead of taking them live through the app) - this ensures that if something goes wrong with the response you still have the image on your camera. The images will also be full quality this way, while taking them through the app may result in reduced quality and image compression. Additionally, make sure your survey is set up to receive and store original quality images - the default is compression so you need to change the default (google search how to do this and provide link here)#todo.\nOffline/Online. Unless you are in a populated area, you may not be able to upload survey responses live in the field. That is fine. Survey 123 allows you to save draft responses to your outbox which are stored locally on your phone. When you get back to a good internet connection, you can submit your survey responses.\nPhone GPS Accuracy. Even if you don’t have service or wifi signal, your phone GPS will be more accurate if you keep your cellular and wifi on. This will drain your battery faster in remote areas but the gain in accuracy is worth it. As mentioned above, for this reason, bring a field ready battery pack.\nGPS Backup. Always ensure that your phone is not your only source of GPS data to insure against data loss. There are several combinations you can use to achieve this:\n\nPhone + Hard Copy. Write down the Lat/Lon on your data sheet or notebook\nTwo Phone apps. Take GPS points with Survey123 & another app such as OnX.\nPhone + GPS unit. Take an indendent GPS unit in the field with you and take a GPS point at every plot or site. Store on the GPS unit.\n\nData Backup. Your data should be backed up each evening when you return from the field. Do this by: exporting images from your phone to a computer or external drive, taking pictures of all hard copy data sheets collected that day and backing them up to a computer or external drive, if you have a wifi connection, uploading tabular data and survey responses to Survey123 and downloading the updated version of the database."
  },
  {
    "objectID": "03a-field-projects.html#raw-data-curation",
    "href": "03a-field-projects.html#raw-data-curation",
    "title": "6  Field Projects",
    "section": "6.3 Raw Data Curation",
    "text": "6.3 Raw Data Curation\nOnce your raw data has been collected, you are ready to curate it. Raw data curation involves assuring that your dataset is complete, formatted correctly, is properly stored and backed up, and includes additional notes not contained in the standard data formats."
  },
  {
    "objectID": "03a-field-projects.html#standards-for-creating-the-raw-data-archive",
    "href": "03a-field-projects.html#standards-for-creating-the-raw-data-archive",
    "title": "6  Field Projects",
    "section": "6.4 Standards for Creating the Raw Data Archive",
    "text": "6.4 Standards for Creating the Raw Data Archive\nYou should strive to meet the following standards when curating your raw data archive:\n\nYour raw data archive should be generated within 1 week of returning from the field (if a single trip), or updated within 1 weeks of collection (for recurring work over longer periods).\nYour raw data archive should consist of:\n\na shapefile in OGC Geopackage format containing lat lon and point or plot IDs\ntabular data exported from Survey123\npictures or scans of your raw data sheets\na local image/video repository where images are placed in a uniqely named folder for each plot or site. NOTE: ideally the title of each folder, in addition to containing the plot or site code should also have a descriptive name alongside the plot code to jog your memory when you are looking back at your plots and images. For example “FRN-31 where we saw moose in lake”.\n\nNOTE: your images should contain metadata in their image names. Nowadays most image files will be exported from your phone with a unique number ID such as “IMG_6249”. You should always retain that number in your file name; BUT you should add additional metadata to the file name by appending: 1) the project id, 2) the plot id, 3) a one or two word phrase regarding the question the image was taken in repsonse to. So, for example, an excellent image title for an image with the name and the folder above would be: “FRN-31_pedon_IMG_6249”. Note that this can be done in an automated way using XLSForms in Survey123. If you want to really take this to the next level, also include some descriptive text in the image file name between the Survey123 generated material and the image file name i.e. “FRN-31_pedon_Bwhorizon30cm_IMG_6249”. *Alternatively, you can also use an open source desktop image metadata editing app such as DigiKam.\n\na file (preferrably in [Markdown]#todo) format which contains any additional qualitative information, notes, observations, or a brain dump from the field data collection - if applicable.\nour standardized [sample log file]#todo that details characteristics and names of physical samples taken.\na readme.txt file that contains metadata about your raw data archive. You can add to your existing readme file or Quarto book as well to include the files in your raw data subdirectory in the readme/metadata.\n\n\nThese files live in the raw data folder of your repo. However note that the image repository will be very large in some cases and gitHub is not a good place to store images. Therefore - in your [.gitignore file]#todo, you should include the image type or the image repository directory so that these do not upload to GitHub.\n\nYour raw data repository should be stored locally and somewhere in the cloud. For everything except your image/video repository should be stored on GitHub. The image/video repository should be stored locally and either in Flickr or a Shared Google Drive (a truly shared drive, not through your personal account) or Google Photos (same re: shared). NOTE #todo: should image repositories also be required to upload to CFANS Drive?\nYour raw data should also contain pre-data - including spreadsheet forms and an archive of the Survey123 form - in what format?#todo.\n\nNOTE: folder repo structure should contain a pre-data folder!!!!!#todo"
  },
  {
    "objectID": "03a-field-projects.html#physical-sample-curation",
    "href": "03a-field-projects.html#physical-sample-curation",
    "title": "6  Field Projects",
    "section": "6.5 Physical Sample Curation",
    "text": "6.5 Physical Sample Curation\nThe short- and long-term curation of physical samples collected during fieldwork is not trivial. There are several issues to be solved - sample names, sample log, and connecting your sample log to a laboratory sample log.\n\n6.5.1 Sample Names\nSee our field manual for detailed protocols on sample naming. Briefly, however, your physical samples should be collected in pre-labelled bags. These bags are pre-labelled with a two digit year and sample number. This allows you to collect samples in the field without labelling bags in difficult conditions. The sample log is then a critical piece of your raw data as it connects your sample numbers to metadata about the sample (depths, plot or site, and horizon, for example). Sample numbers then be something like “22-001”. NOTE; this assumes that you won’t collect more than 999 samples in a year (usually safe?). If projects are occuring at the same time, then we need a way of differentiating between these samples. We should therefore append the sample number with the last initials of the project lead who is collecting them in the field (i.e. “22-001-NAJ”).\n*NOTE: if you are working on an externally sponsored project or a project that has a larger collaborative team or more parts, your sample names may be prescribed for you. That is fine - in that case, use the sample naming conventions that are part of the larger project. - how will this work with master sample log integration?\n\n\n6.5.2 Sample Log\nUse the sample log template provided in our Field Manual. Take pictures and backup pictures or scans of the sample log each night after data collection.\n\n\n6.5.3 Connecting your sample log to a master laboratory sample log #todo\nBig idea - use Make (Integromat) to trigger a watch on a Survey123 survey that automatically appends row (note need to have survey set up with repeat to produce multiple records of samples with one survey record submission) to a master laboratory physical sample sheet. The Google sheet should also somehow provide a sample name for each? Or you could have in your question at the site or plot level with multipl entries - what are the names/IDs of the samples collected at this plot/site? YES, this second is the best way to go.\n\n\n6.5.4 Subsequent Laboratory Analysis of Physical Samples #todo\nThis should maybe be a separate chapter after lab analysis. then can be two stages - integrate with Google Sheets, pull from physical sample log, build on that."
  },
  {
    "objectID": "03b-lab-projects.html#pre-data-infrastructure",
    "href": "03b-lab-projects.html#pre-data-infrastructure",
    "title": "7  Laboratory Projects",
    "section": "7.1 Pre-Data Infrastructure",
    "text": "7.1 Pre-Data Infrastructure\n\n7.1.1 Google Sheets Integration with Master Data Sheet Using Make and R\nThe primary data entry location for laboratory data is Google Sheets. NOTE: your Google Drive folders for your project should be configured exactly the same as your local PARA repository. Since you are using GitHub there doens’t need to be an extra full copy of the repo on Google Drive, so there will be a lot of empty folders - but that is fine. Set up a Google Sheet in your raw data folder for the project on Google Drive. Then, using the googlesheet4 package in R, [pull down a copy of the lab master sample log]#todo, [subset it for the samples in your project]#todo, and export that subset to another google sheet. Now you can add fields to the Google Sheet to populate them as you receive data or conduct laboratory analysis. *NOTE: once you have your project subsetted sample log you can create a field which contains an internal “lab” ID, which is usually a short integer and can be convenient when labelling tubes, etc. However, only include these short lab IDs on your project specific sample log. Do not include them on the laboratory master sample sheet.\nNOTE: if you are analyzing samples in your project that were not physically collected by you (but were collected by collaborators as part of this or another project), you should manually enter those samples into the master lab sample log and utilize the sample IDs provided by the collaborators.\n\n\n7.1.2 Self-Generated Data: Laboratory Notebook and Analysis or Task Log\nFor data that you are generating yourself, you should keep a [Laboratory Notebook] #todo - see standards and templates in our Laboratory Manual. If you are doing an analysis that results in manually recorded data (such as pH, which requires you to look at a screen and writ ethe number down in your lab notebook), data is then transferred manually from your laboratory notebook to the Google Sheet and fields are added as needed. If you are doing an analysis that results in digital data (such as pXRF, MIR, or LPSA), then you will need to export the digital data and populate it into the correct fields associated with the samples in your data sheet.\n\n\n7.1.3 Externally Generated Data: Original Data File + Integration\nFor externally generated data (i.e. you sent samples off to a commerical lab and recieved a pdf or tabular report back). you will need to export the digital data and populate it into the correct fields associated with the samples in your data sheet. NOTE that all original data files from external sources should be kept (as they were received) in a subfolder of the raw-data subdirectory."
  },
  {
    "objectID": "03b-lab-projects.html#curating-laboratory-data",
    "href": "03b-lab-projects.html#curating-laboratory-data",
    "title": "7  Laboratory Projects",
    "section": "7.2 Curating Laboratory Data",
    "text": "7.2 Curating Laboratory Data\nLab Data curation involves ensuring that all self- or externally-generated data is kept in its original form as well as transferred to the project sample log/data sheet which is the foundational raw data sheet for the project.\n\n7.2.1 Raw Data Repository Standards for Laboratory Projects\nYour raw-data repository for Laboratory Projects should include the following:\n\nThe subsetted sample log w/ fields added and updated according to the [Jelinski Lab Tabular Data Standards] #todo\nImages or scans of a laboratory notebook\nOcassionally, your laboratory analysis may result in physical products such as soil chromatographs. These should be stored in their original state, scanned in a standardized way, and the image repository should be handled in the same way as image repositories are handled for field projects.\na readme or metadata file (or chapter in your metadata Quarto book). The metadata file should contain references, either to our internal lab protocols documented in our lab manual #todo or to published or available protocols from other sources.\n\nNOTE: see the laboratory methods manual for standardized protocols on how to maintain a laboratory notebook, and a sample analysis/task log.\nThese files live in the raw data folder of your repo. However note that if you have images, the image repository will be very large in some cases and gitHub is not a good place to store images. Therefore - in your [.gitignore file]#todo, you should include the image type or the image repository directory so that these do not upload to GitHub.\n\nYour raw data repository should be stored locally and somewhere in the cloud. For everything except your image/video repository should be stored on GitHub. The image/video repository should be stored locally and either in Flickr or a Shared Google Drive (a truly shared drive, not through your personal account) or Google Photos (same re: shared). NOTE #todo: should image repositories also be required to upload to CFANS Drive?\nYour raw data should also contain pre-data - what is pre-data in this case? #todo - in what format?#todo.\n\nNOTE: folder repo structure should contain a pre-data folder!!!!!#todo\nNOTES to add: #how deal with replicate sample runs.# flagging samples for reruns"
  },
  {
    "objectID": "03c-review-projects.html#pre-data-infrastructure",
    "href": "03c-review-projects.html#pre-data-infrastructure",
    "title": "8  Review Projects #todo",
    "section": "8.1 Pre-Data Infrastructure",
    "text": "8.1 Pre-Data Infrastructure\n\n8.1.1 Resources for determining review type and protocol\nShould consult with Hava here or ask her to contribute as she has become an expert.\nFor now just providing a few resources:\n\n[SciSpace Literature Tools Review] (https://typeset.io/resources/online-tools-for-easy-literature-review/#:~:text=5%20Tools%20for%20Easy%20Literature,Guides%20%26%20eBooks)\nUMN Libraries Systematic Review and Evidence Syntheses Landing Page\nGore and Boruff - McGill University, Systematic Reviews, Scoping Reviews, and other Knowledge Syntheses"
  },
  {
    "objectID": "03d-data-synthesis-projects.html#pre-data-infrastructure",
    "href": "03d-data-synthesis-projects.html#pre-data-infrastructure",
    "title": "9  Data Synthesis Projects #todo - ask Hava for opinions as she is an expert",
    "section": "9.1 Pre-Data Infrastructure",
    "text": "9.1 Pre-Data Infrastructure\n\n9.1.1 Selecting and documenting a data platform\nBecause data synthesis projects involve consolidating and wrangling of data from many different sources, it is important to choose a data “platform” that will support consolidation and curation of this data.\nThis data platform could be as simple as a well documented folder structure on your local computer, or more complex such as the UMN GEMS data platform, which provides a workspace for uploading datasets, curating dataset and field metadata, controlled vocabularies and ontologies, and post curation scripting for data harmonization.\nAll projects should include a data management plan (as a default you can use our template, developed from this workflow and located here #todo) however, data synthesis projects in particular require significant metadata documentation. As with other project types, this project-level metadata documentation should begin on project creation and continue during the pre-data phase, so that the documentation is well developed prior to data acquisition.\n\n\n9.1.2 Selecting and documenting data harmonization procedures\nData synthesis projects require some level of harmonization that allows comparison of similar data types between diverse datasets. At a high level, this harmonization requires understanding similar fields between different datasets, despite very different data structures and field naming conventions. It is important that this field data curation occurs in conjunction with field and dataset metadata in the original datasets.\nThere are two main approaches to data harmonization (may not be using the right terms here #todo):\n\nPre-ingestion harmonization\nPost-ingestion harmonization\n\nNOTE: see and cite the work of Kathe Todd-Brown and others here - include Hava’s expertise.\nThe advantages and disadvantages of each approach are as follows:\n\nPre-ingestion harmonization The advantage of pre-ingestion harmonization is that the datasets are manually placed in a single, easy to work with format. There are major disadvantages though. Fields are renamed from the original source with little to no record or mapping back to the original field in the original source.\nPost ingestion harmonization The advantage of post-ingestion harmonization is that raw datasets are retained on the data platform in their original form. Provided you have a data platform infrastructure that has advanced capabilities such as GEMS, field metadata tags can be added on top of the original field names in each dataset which both preserves the fidelity of the original data and provides mapping from terms and field names in the original data to a controlled vocabulary or ontology developed and documented in the context of the project."
  },
  {
    "objectID": "03d-data-synthesis-projects.html#curating-raw-data-in-a-data-synthesis-project",
    "href": "03d-data-synthesis-projects.html#curating-raw-data-in-a-data-synthesis-project",
    "title": "9  Data Synthesis Projects #todo - ask Hava for opinions as she is an expert",
    "section": "9.2 Curating Raw Data in a Data Synthesis Project",
    "text": "9.2 Curating Raw Data in a Data Synthesis Project\nData Synthesis curation involves ensuring that all datasets are kept in their original form along with their original metadata and links or references to the original source material or the data in the data repository.\n\n9.2.1 Raw Data Repository Standards for Data Synthesis Projects\nYour raw-data repository for Laboratory Projects should include the following:\n\nData from the original sources in its original form, regardless of whether or not that form is in the acceptable standardized format for the data platform. # note that not all platforms require data to be in the same format prior to ingestion. Flexible platforms such as GEMS allow all types of datasets, and then format conversion can be accomplished as part of the scripting process.\nAll metadata associated with original datasets in its original form. This metadata should include links back to the original sources, references, or data repositories.\nProject metadata documentation, including information about data platform capabilities and harmonization procedures. This should include information about any controlled vocabularies or ontologies built or utilized.\n\n\n\n9.2.2 Other considerations #todo\n\ndata sharing agreement for each dataset contributed\nco-authorship guidelines # how far do they go?\nprivate or public options for each dataset"
  },
  {
    "objectID": "04-raw-data-qaqc.html",
    "href": "04-raw-data-qaqc.html",
    "title": "Quality Assurance (QA) and Quality Control (QC) on Raw Data",
    "section": "",
    "text": "Differences between Quality Assurance (QA) and Quality Control (QC)\nQuality assurance (QA) is the deployment of protocols and methods to prevent defective or erroneous data from entering your workflow during data collection, whereas quality control (QC) is the detection of erroneous or defective data in an existing dataset USGS::Data Management - Manage Quality. For this reason, quality assurance (QA) processes are best integrated into this workflow at the “pre-data” and raw data collection stages, while quality control (QC) is the first part of the data munging or data wrangling process.\n\n\nQuality assurance (QA) planning and practice\nQA planning intersects significantly with your pre-data workflow and may be a part of any data management plan. QA processes for a project are best articulated in a Quality Assurance Plan (QAP). One of the best resources for QA planning is the USGS Quality Assurance Plans: Recommended Practices and Examples resource. This resource recommends the following components of a QAP:\n\nIdentif[ies] data quality objectives for your data or project\nIdentif[ies] requirements for\n\nStaff skills and training\nField and lab methods and equipment that meet data-collection standards\nSoftware and file types to use for data handling and analysis that support data quality goals\nData standards, structure, and domains consistent with community conventions for other data in the same subject area\nPeriodic data-quality assessment using defined quality metrics\n\nDescribe[s] a structure for data storage that can also facilitate checking for errors and help to document data quality\nDescribe[s] approved data entry tools and procedures, when applicable\nEstablish[es] data-quality criteria and data-screening processes for all of the data you will collect\nInclude[s] quality metrics that can determine current data-quality status\nEstablish[es] a plan for ‘data quality assessments’ as part of the data flow\nContain[s] a process for handling data corrections\nContain[s] a process for data users to dispute and correct data\n\nQA practice is the application of QAP components to prevent the creation of erroneous or defective data whenever and wherever possible.\nThis includes:\n\nciting well documented methods for data collection OR developing detailed documentation for an new data collection methods or protocols\ndeveloping training checklists and standards for staff or scientists involved in data collection\nfor laboratory procedures: ensure that methods for including blanks, replicates, and standards are well documented and standardized\nfor laboratory procedures: initial review of standards, replicates and blanks to ensure a sample, set of samples or sample run has passed the minimum acceptable standard for inclusion into the dataset\ncould involve the coding of quality review flags if data is input through automated forms\nprotocols and standards for transferring information from physical data sheets to electronic data sheets to detect and avoid errors in data input\nprotocols and standards for versioning of shared data sheets to avoid conflicts and input errors\n\n\n\nQuality Control (QC) practice\nQC in practice is best integrated into the initial portion of the data munging stage of the workflow and will be addressed in that portion of this document.\n\n\nAdditional QA/QC Resources\nUSGS:: Data Management - Manage Quality USGS :: Data Management - Quality Assurance Plans: Recommended Practices and Examples USGS:: Data Management - QA - Preventing Data Issues: Recommended Practices Arthur Chapman :: Principles of Data Quality"
  },
  {
    "objectID": "05-data-munging.html#documenting-data-munging---standards",
    "href": "05-data-munging.html#documenting-data-munging---standards",
    "title": "Data Munging -also called data wrangling",
    "section": "Documenting Data Munging - Standards",
    "text": "Documenting Data Munging - Standards\nAs R is the main language of choice in our lab, the default data munging documentation standard involves an R project or series of R scripts. These should be stored in the src folder of your project directory and curated according to our lab [R code and project style guidelines]#todo.\nData munging documentation should also include metadata for this stage - and could be in the form of part of a project Quarto book published on GitHub Pages. This metadata should document all decisions made in the data munging process, the why and how. Due to the detailed nature of many of these decisions they are too much detail for a standard readme file or a project metadata file. However, a Quarto book with sections, chapters, and space to provide R code blocks would be an excellent way to document these decisions.\n\n\n\n\n\n\nRequired Tools - Data Munging\n\n\n\n\nR/RStudio\nGoogle Drive"
  },
  {
    "objectID": "05a-pulling-raw-data.html#where-to-put-your-raw-data",
    "href": "05a-pulling-raw-data.html#where-to-put-your-raw-data",
    "title": "10  Reproducibly Pulling Raw Data",
    "section": "10.1 Where to put your raw data",
    "text": "10.1 Where to put your raw data\nAny raw data you generated should live in the following place in our standardized project repository:\n./00-data/a-raw\nAny data in raw data that is not your own and IS NOT findable by DOI or in an existing long term data repository should either be in an /external-sources subdirectory of the ./00-data/a-raw/ directory, which contains a metadata file that details the source of each of the external data sources. Raw data that IS findable in an existing long-term data repository by DOI should be reproducibly pulled in by script (see below)."
  },
  {
    "objectID": "05a-pulling-raw-data.html#how-to-reproducibly-pull-raw-data-into-your-workflow",
    "href": "05a-pulling-raw-data.html#how-to-reproducibly-pull-raw-data-into-your-workflow",
    "title": "10  Reproducibly Pulling Raw Data",
    "section": "10.2 How to reproducibly pull raw data into your workflow",
    "text": "10.2 How to reproducibly pull raw data into your workflow\nThe way in which you reproducibly pull raw data into your workflow depends on whether: 1) you generated it or it is an external datasource that is NOT openly finadable and accessible by DOI or 2) it IS an external datasource that is openly accessible and findable by DOI.\n\n10.2.1 Reproducibly pulling data that is your own or is not accessible by DOI\nEven though the project directory template already contains a .Rproj file, you should always use the here::here function in the here package in R when referencing file locations in your project repo - there are excellent reasons for using both .Rproj and here::here - when combined these are a powerful way to guarantee there won’t be any issues with other users running your code due to directory issues. Malcom Barrett gave an an excellent talk at the useR! 2020 conference entitled “Why use the here package when you’re already using projects?”. The .Rproj file contained in this tenplate is pre-configured to start with a completely clean workspace EVERY TIME by selecting in Project Options &gt; General “Restore .RData into workspace at startup” = NO and “Save workspace to .RData on exit” = NO, “Disable .Pprofile execution on session start/resume” = CHECKED, “Quit child processes on exit” = CHECKED. These two options combined will also ensure that others running your code (or even you at a later date) don’t experience errors or conflictions due to workspace-specific background - this is often the cause of the following issue: “I swear…my code worked the last time I ran it and I didn’t change anything!”. This will also avoid Jenny Bryan coming into your office and setting your computer on fire.\n\n\n10.2.2 Reproducibly pulling data that is not your own or is accessible by DOI\nAny data sources required for your workflow that are open and accessible by DOI should be pulled into your workflow reproducibly by using the download.file and if necessary the unzip functions in R utils. This data should be placed in either the ./00-data/a-raw folder if it needs to be further munged or in the ./00-data/b-prepared/ folder if it is ready to be used immediately in your workflow.\n\n\n10.2.3 Tidyverse Tibbles or Dataframes when importing non-spatial data?\nOur standard format for non-spatial data is a .csv. There are two ways to import .csv files into your workflow: base::read.csv and [tidyverse::readr::read_csv]( Although many folks have converted completely to tibbles I’m not quite sure there is a black and white answer to this and it depends a bit on your workflow and previous R experience. I still use dataframes but might be rapidly becoming a dinosaur. There are a lot of advantages to tibbles, however one potential disadvantage is some of their workability related to arithmetic operations and assigning variable types. However, Tidyverse and Tibble adoption has increased dramatically and Tibbles, strictly speaking provide more transparent and stricter controls over data formats than data frames. Tibbles can easily be converted to data frames and vice versa. And there are very few if any downsides to using tibbles instead of dataframes (mostly because tibbles are actually still dataframes) - the only real downsides are that some older packages may not play well with tibbles, but then you can always just convert them if necessary!"
  },
  {
    "objectID": "05a-pulling-raw-data.html#additional-resources",
    "href": "05a-pulling-raw-data.html#additional-resources",
    "title": "10  Reproducibly Pulling Raw Data",
    "section": "10.3 Additional Resources",
    "text": "10.3 Additional Resources\nAmy Johnson::R training for Stanford Library Software Services and Data Science (SSDS) Group - Data Jingfei Fang::Tibble vs Data Frame Josh Gonzales::R Functions:read_csv() CRAN R Project::Tibbles Jumping Rivers::The Trouble with Tibbles Hadley Wickham::tibble 1.0.0 Hadley Wickham::R for Data Science (2e) - Whole Game - Data import Software Carpentry::Programming with R - Reading and writing csv files tibble.tidyverse.org::tibble 3.2.1\n\n\n\n\n\n\nRequired Tools - Data Munging\n\n\n\n\nR/RStudio"
  },
  {
    "objectID": "05b-qc-data-cleaning.html#qc-and-data-cleaning-processes",
    "href": "05b-qc-data-cleaning.html#qc-and-data-cleaning-processes",
    "title": "11  Quality Control (QC) and Data Cleaning",
    "section": "11.1 QC and Data Cleaning Processes",
    "text": "11.1 QC and Data Cleaning Processes\nOnce your raw data has been pulled into your workspace in a reproducible way, it is important to conduct QC and data cleaning in order to detect any potentially erroneous, defective, or missing data and subsequently decide how to proceed. While the detection process is more amenable to automated procedures and can be quantified, deciding how to proceed once these potentially defective data points are detected can be subjective and requires domain knowledge as well as intuition, along with rich and reproducible documentation of decisions made. The following standard QC procedures should be performed on every dataset:\n\n11.1.1 Basic formatting and QA follow-up: Structural QC\nAlthough a good QA plan and processes can ensure that the raw data conforms to standards and contains as few structural errors as possible, the first step in any QC process should be structural. Some of these structural issues can be resolved in the read-in process with additional arguments to read.csv(imports data as data frame), tidyverse::readr::read_csv(imports data as tibble) or other preferred functions in raw data import. Others may involve additional scripting to resolve:\n\nAre non-spatial data files in text format (separated values, preferrably csv as per our lab standards)?\n\nIf they are not, write a script that converts the raw data file from non .csv format to .csv format.\n\nDoes the first row contain column names?\n\nIf not - assign column names using colnames or setnames.\n\nAre there special characters, spaces or other wierdness in the column names?\n\nIf so, rename them as above\n\nAre data types or column classes correct?\n\nR has 5 basic data types: character (text string), numeric (real or decimal), integer, logical (true/false) and complex (numbers with real and imaginary parts). For the most part we rarely encounter complex data types in our line of work, and we seldom use logical data types (true/false) - preferring instead to code those as binary 1/0. Additionally, because numeric data types also include integers, it is often just fine for integers to be numeric - however there may be some cases when you want to enforce integers strictly, in which case you do want to use the integer class. SO - this means that you really just need to be concerned about whether character and numeric data types are assigned as expected in your imported dataset. NOTE: R will automatically detect data types on import unless explicitly assigned - this is fine most of the time and actually worksquite well for QC purposes because, for example, if a field (column) that you expect to be numeric (such as say, pH) is listed as character - there are likely some unwanted values in your dataset for that field that you want to investigate. NA values are read as NULL or missing and therefore do not affect variable type. However, if in your pH column you have a value such as 5.4x or &lt; 5.4 or N/A, then the entire field will be read as a character. You need to go in and fix that. One way of doing this is to display unique values for the field using the base::unique function and searching for any unexpected values. You can then replace or correct these values using the base::replace function ON THE FIELD OF INTEREST (preferred) or on the whole dataset. Once you have resolved any of these outstanding issues, you can coerce the field to the data type of your choice using as.character, as.numeric.\n\nAre there empty columns or rows in the dataset?\n\nYou may have some completely empty or null columns or rows in your dataset. Most of the time you may want to remove these. There are a few ways to go about it:\n\nuse a custom function\nuse the janitor::remove_empty function\nusing colsums, sapply or tidyverse::discard\n\n\n\n\n\n11.1.2 Domain QC\nOther QC issues typically involve applying domain knowledge to make some objective decisions regarding QC and data cleaning. These are: - ID’ing and deciding what to do with missing values, - Identifying and deciding what to do with duplicates - Identifying and deciding what to do with potential outliers - Deciding when/if actors vs character data types are most appropriate: Factors vs characters and\n\n11.1.2.1 Identification of missing values, review and decisions\nThe identification of missing values is a critical first step in QA and data cleaning. This identification should be done programmatically, and once ID is complete, decisions should be made regarding how to deal with these values. There are several options:\n\nRetain missing values as is\nReplace missing values with “imputed” values\nDelete records with a high proportion of missing values\n\n\n11.1.2.1.1 Retaining missing values\nGiven the high uncertainty of many environmental measurements, it is often preferrable to retain missing values in the dataset. In practice, using our R-based workflow, this means that when univariate, bivariate, or multivariate analyses are performed on the dataset that call a variable for which a particular sample has missing values - these samples will not be"
  },
  {
    "objectID": "05b-qc-data-cleaning.html#additional-resources",
    "href": "05b-qc-data-cleaning.html#additional-resources",
    "title": "11  Quality Control (QC) and Data Cleaning",
    "section": "11.2 Additional Resources",
    "text": "11.2 Additional Resources"
  },
  {
    "objectID": "05b-qc-data-cleaning.html#section",
    "href": "05b-qc-data-cleaning.html#section",
    "title": "11  Quality Control (QC) and Data Cleaning",
    "section": "11.3 ",
    "text": "11.3 \n\n\n\n\n\n\nRequired Tools - Data Munging\n\n\n\n\nR/RStudio"
  },
  {
    "objectID": "05c-standardizing-formats.html#jelinski-lab-standard-data-formats",
    "href": "05c-standardizing-formats.html#jelinski-lab-standard-data-formats",
    "title": "12  Standardizing Formats #todo",
    "section": "12.1 Jelinski Lab Standard Data Formats",
    "text": "12.1 Jelinski Lab Standard Data Formats\n\n12.1.1 Tabular Data\nAll raw tabular data should be converted to .csv format. .csv is a simple text file that allows data to be stored in text form but readable by R, Excel and many other programs as a spreadsheet. In a .csv file, commas separate cells within a row and returns (new lines) indicate a new row. Tabular data in text form with other delimiters (such as tab, space or custom delimiters) can be converted to .csv formats easily in R.\nNote that one disadvantage of .csv formats is that data types which are stored in databases (such as dataframes in R or in other databases) are not retained. The data is text only. For most projects this is not an issue since R will automatically detect data types upon ingestion, but for some specialized projects database to database transfers might be better a better option to retain this information if necessary.\n\n\n12.1.2 Spatial data\nVector data (points, lines and polygons) should be converted to OGC Geopackage format. This can be easily accomplished in the terra package in R.\nRaster data (cell-based information) should be converted to GeoTIFF format.\nNote that these conversions can also occur as part of larger scripts in workflows, but should be documented clearly.\n\n\n12.1.3 Images\nImages should be converted to .jpeg format without compression so that the original image quality is retained. If you are taking pictures on an iPhone, the raw exported format will be Apple’s proprietary .HEIC format which is readable on Apple devices but not Windows devices. .jpeg is currently the most widely used and future proof image format. #todo - add something about RAW formats from DSLRs.\nThese conversions need to occur with a manual workflow using a Native application such as Preview or a proprietary application such as Photoshop.\n\n\n12.1.4 Videos\n#todo - .mpeg format?"
  },
  {
    "objectID": "09-style-guides-and-standards.html",
    "href": "09-style-guides-and-standards.html",
    "title": "Jelinski Lab Style Guides and Standards",
    "section": "",
    "text": "This section details our current style guides and standards for R scripts, markdown, and tabular data."
  },
  {
    "objectID": "09a-r-style-guide.html#naming-things",
    "href": "09a-r-style-guide.html#naming-things",
    "title": "13  Jelinski Lab R code Style Guide and Standards",
    "section": "13.1 Naming things",
    "text": "13.1 Naming things\nNaming things is hard - one of the hardest parts of scripting. Below are the guidelines you should use for naming things in your R code as a memebr of my lab group. These are not absolute rules, but rather guidelines that will ensure all of our code is consistent and is readable by all group members. Here are some general guidelines before I get to the specific styles below. These guidelines are from Robert Martin’s Clean Code: A Handbook of Agile Software Craftsmanship (2009)\n\nDon’t use names of established objects, classes or functions\nMake Meaningful Distinctions\nAvoid Redundancy\nUse pronounceable names\nUse searchable names\nDon’t be cute\nPick one word for an abstract concept and stick to it - i.e. don’t use synonyms - instead add a description in the name after that word; i.e. “get_type”, “get_value”, etc.\nDon’t add gratuitous context or prefixes\n\n\n13.1.1 R scripts\nThese files end in .R. They should contain NO special characters !, @, #, $, %, ^, &, *, (, ) or spaces. Only lowercase letters (no capital letters), -, and _. Additionally, I highly recommend adding numbers to your R scripts within projects (see section X.X, above. This helps both with file name recall and for you and/or a user to understand a sequence for running the scripts (if there is one). Left pad the numbers with a zero to allow you to go up to 99. The “00” script should be reserved for a master script that runs all of the other scripts for a project within it. The name for your script should be descriptive, and preferably 2-4 words in length. However if a longer name provides a better description or more comprehensive readability do not hesitate to use it!\n# Good\n00_master_script.R\n01_load_baseline_data.R\n\n# Bad \n00 Master!.R\n01LOAD.R\n\n\n13.1.2 Functions\nFor functions that you write (i.e. private functions), use .BigCamelCase. The camel case clearly differentiates functions from other named things (objects and scripts), and the dot at the beginning clearly identifies the function as a private function, and not one being called from Base R or an existing package.\n# Good\n.DoNothingPrivately &lt;- function() {\n  return(invisible(NULL))\n}\n\n# Bad\ndo_nothing &lt;- function() {\n  return(invisible(NULL))\n}\nR.C. Martin (2009) suggests the following with regard to writing functions. Many of these are useful, some irrelevant in 2023, but all good things to think about if you are writing your own functions: - Functions should be small, less than 20 lines max, and even smaller than that if possible. - Blocks and indenting should be used - Functions should do one thing - “if a function does only the step that are one level of abstraction below the stated name of the function, then the function does one thing”. - another way “to know that a function is doing more than one thing is if you can extract another function from it with a name that is not merely a restatement of its implementation” - One level of abstraction per function - what is a “level of abstraction”? - the stepdown rule - read code from top to bottom - human readable: to blank then we need to blank, to blank then we need to blank, etc - see page 113 - Use Descriptive Names - Try to have two arguments per function max\n\n\n13.1.3 Objects\nObject names are assigned within an R script. Similar to script names, they should contain NO special characters !, @, #, $, %, ^, &, *, (, ) or spaces. Only lowercase letters (no capital letters), -, and _. Unlike script names, however, they do not begin with numbers. The name for your objects should be descriptive, and preferrably 2-4 words in length. However if a longer object name provides a better description or more comprehensive readability you may use it! Just be aware there is a dichotomy here - unlike script names, objects tend to be called and referred to multiple times and therefore will show up in your code multiple times. Long and/or tedious object names can be difficult to maintain. So think carefully about these. I think naming objects is the hardest. also, name linear models with a .lm at the end etc. Plots )or saved plot components end in .p\n# Good\npf_binary\nannual_data\n\n# Bad \nPFbinary\nannual data"
  },
  {
    "objectID": "09a-r-style-guide.html#code-length-punctuation-and-readability",
    "href": "09a-r-style-guide.html#code-length-punctuation-and-readability",
    "title": "13  Jelinski Lab R code Style Guide and Standards",
    "section": "13.2 Code length, punctuation, and readability",
    "text": "13.2 Code length, punctuation, and readability\n\n13.2.1 Commas, parentheses, spaces, operators\nUse commas and parentheses as in normal English. Always use a single space after a comma…\n# Good\n[, 1]\n\n# Bad\n[,1]\n…no spaces before or after (inside or outside) parentheses…\n# Good\nmean(x, f)\n\n# Bad\nmean( x, f )\n…except for when () is used for function arguments.\n# Good\nfunction(x) {}\n\n# Bad\nfunction (x) {}\nfunction(x){}\n“Infix” operators (operators typically used between two objects or text strings, i.e. (=, ==, +, -, &lt;-, etc.)) should be surrounded by spaces….\n# Good\nheight &lt;- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\nheight&lt;-feet*12+inches\nmean(x, na.rm=TRUE)\n…however, operators with high precedence :, ::, :::, $, @, [, [[, ^, etc. should never be surrounded by spaces.\n# Good\nsqrt(x^2 + y^2)\ndf$z\nx &lt;- 1:10\n\n# Bad\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx &lt;- 1 : 10\n\n\n13.2.2 Line breaks and length\nneed to add"
  },
  {
    "objectID": "09a-r-style-guide.html#code-sections-comments-fences-folds-and-scripts.",
    "href": "09a-r-style-guide.html#code-sections-comments-fences-folds-and-scripts.",
    "title": "13  Jelinski Lab R code Style Guide and Standards",
    "section": "13.3 Code sections, comments, fences, folds and scripts.",
    "text": "13.3 Code sections, comments, fences, folds and scripts.\n\nBreak your workflow up into numbered scripts.\nWithin each numbered script, you should have code sections that are fenced off that delineate major sections within that script.\nStandard for fences is 4 hashes to start followed by 4 hashes i.e.: #### Fence section name ####\nThis ensures foldability, etc. Fenced section names start with a capital letter and are then all lowercase. No period at the end.\nEach function you write should each be in a single script, unnumbered, but titled by the function name.\nEach line of a ==comment== should begin with the ==comment== symbol and a single space: # then should begin with lowercase, all lowercase, no period at the end. Comments go above the code that they run."
  },
  {
    "objectID": "09a-r-style-guide.html#examples.",
    "href": "09a-r-style-guide.html#examples.",
    "title": "13  Jelinski Lab R code Style Guide and Standards",
    "section": "13.4 Examples.",
    "text": "13.4 Examples."
  },
  {
    "objectID": "11-summary.html",
    "href": "11-summary.html",
    "title": "14  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "workshop-18DEC2023-part1-overview-github.html#introduction",
    "href": "workshop-18DEC2023-part1-overview-github.html#introduction",
    "title": "15  Part 1: Overview and Setting up GitHub",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nThe workshop leader, Nic, explains that the goal of the workshop is to get everyone in the lab group working in a standardized and efficient way on research projects. The aims are to facilitate open and reproducible science, collaboration, and time management.\n\n15.1.1 General Philosophy\nThe philosophy behind the workflow is “radical open science” - showing work publicly while it is happening rather than waiting until it is published. There can be reasons to keep some data private temporarily, but openness should be the default.\nNic argues we are beyond the days where work should be kept hidden until publication. There are benefits to showing work in progress, though judgments have to be made about how much to show at what stages.\n\n\n15.1.2 Defining Projects\nIn academia, the core project units that people are assessed on are papers, proposals, dissertations and theses. So the workflow is oriented around facilitating progress on those outputs.\nThe aim is to have a system where all information related to a project lives in one place from the start, with clear naming conventions. This avoids data and files getting scattered across multiple platforms. It also makes projects transferable between people and platforms.\n\n\n15.1.3 Lab Github Organization\nThe group uses a shared Github organization for open lab resources:\n\nOpen science guidebook\nLab protocols eBook\nGroup handbook (onboarding etc)\nProject template\n\nNic explains Github repositories and branches for managing versions and collaborative work."
  },
  {
    "objectID": "workshop-18DEC2023-part1-overview-github.html#registering-projects",
    "href": "workshop-18DEC2023-part1-overview-github.html#registering-projects",
    "title": "15  Part 1: Overview and Setting up GitHub",
    "section": "15.2 Registering Projects",
    "text": "15.2 Registering Projects\nThe first activity is for each person to identify their current paper projects - anything from an initial idea to near submission.\nNic introduces the project registry - a table for tracking all lab projects in a standardized way. Everyone registers one of their projects by adding to the table.\nThe registry assigns a unique numbered code to each project. This ties together related files/folders across platforms and over time as titles change.\nParticipants add their name, project title, status (idea, analyzing data etc.)\n\n15.2.1 Step-by-step instructions for registering projects\n\nIdentify your potential paper projects - these can range from initial ideas to near complete drafts. Come up with a brief 1-2 sentence description for each project.\nGo to the Lab Group’s GitHub organization page and click on the Project Registry.\nClick “Add Item” at the bottom of the Project Registry table to add a row.\nIn the new row, fill in the following columns:\n\n\nNumber: Select the next available project number to uniquely identify your project\nName: Use the naming convention “Number-YourLastName-2-4WordDescription” e.g. “065-Jelinski-LakeSedimentCoreAnalysis”\nStatus: Select the current status of your project idea, analysis, drafting etc.\n\n\nMake note of the unique number and name you entered. This will be used to identify all folders and files associated with your project across platforms."
  },
  {
    "objectID": "workshop-18DEC2023-part1-overview-github.html#setting-up-project-repositories",
    "href": "workshop-18DEC2023-part1-overview-github.html#setting-up-project-repositories",
    "title": "15  Part 1: Overview and Setting up GitHub",
    "section": "15.3 Setting up Project Repositories",
    "text": "15.3 Setting up Project Repositories\nNext, participants use the Project Template repository to create a repository for their registered project within their personal Github account.\nThis automatically structures their project folder with:\n\nDetailed readme\nFolders for data, documents etc\nTemplate markdown documents\n\nNic explains the contents and how to customize.\nThe readme and structure provide a standardized way for people to organize their projects as they get started. They can modify as needed over time.\n\n15.3.1 Step-by-step Instructions for Setting Up Your Repository\n\nGo back to the Main Jelinski Lab GitHub Organzation page and click on the Project Template repository.\nClick on “Use this template” and select “Create new repository”\nFor Owner, select your own GitHub account.\nFor Repository name, enter the unique name you created when registering your project e.g. 065-Jelinski-LakeSedimentCoreAnalysis\nYou can choose Public or Private visibility. You can change this later.\nClick “Create repository” from template. This will set up a repository in your GitHub account containing folders, templates and documentation to standardize your project workflow."
  },
  {
    "objectID": "workshop-18DEC2023-part1-overview-github.html#next-steps",
    "href": "workshop-18DEC2023-part1-overview-github.html#next-steps",
    "title": "15  Part 1: Overview and Setting up GitHub",
    "section": "15.4 Next Steps",
    "text": "15.4 Next Steps\nIn the final segments, Nic aims to show everyone how to:\n\nGenerate a Quarto ebook from their markdown documents and connect it to their Github repository\nUse logging conventions to track status and notes\n\nThere will also be discussion of file naming conventions and digital organization systems."
  },
  {
    "objectID": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html",
    "href": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html",
    "title": "16  Part 2: Using GitHub Desktop, Local Workflow, R Reproducibility, Zotero",
    "section": "",
    "text": "17 Summary\nIn summary, Nic covered setting up an open science Github repository, demonstrated workflows for version controlling and sharing analysis via Github Desktop, shared best practices in R for reproducible code, strategies for handling large data files, and utilizing Zotero to collaboratively manage references and PDF literature."
  },
  {
    "objectID": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#overview",
    "href": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#overview",
    "title": "16  Part 2: Using GitHub Desktop, Local Workflow, R Reproducibility, Zotero",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nThis section of the transcript covers setting up Github repos, workflows for version control and collaboration, best practices for reproducibility in R projects, handling large files, and utilizing Zotero for reference management and PDF storage.\n\n16.1.1 Setting up a Github Repo to Work with Github Desktop\nNic demonstrates creating a Github repo from a template he designed for open science workflows. The repo contains a standardized folder structure for organizing code, data, outputs, manuscripts, presentations, metadata, support files, and documentation. It also includes helper files like a gitignore, license, readme, and R project file.\nAfter creating the repo, Nic shows how to clone it to your local machine using Github Desktop. He renames the R Project file to match his new repo name. In Github Desktop, this change is detected, allowing him to commit and push the updates back to the main branch on Github.\n\n16.1.1.1 Step-by-step instructions for working with GitHub Desktop, GitHub, and your local machine\n\n16.1.1.1.1 Cloning the Repo Locally with GitHub Desktop\n\nDownload and install GitHub Desktop if not already: https://desktop.github.com/\nOpen GitHub Desktop and log into your GitHub account\nClick File &gt; Clone Repository\nSelect the repository you created from the list\n\nSelect a Local Path to store the cloned repo on your computer\nClick Clone\nThe repo will now be cloned onto your computer linked to the GitHub remote repository\n\nNow you can develop locally and use GitHub Desktop to push changes:\n\nModify files and work on the project on your computer\n\nGo to GitHub Desktop and check the changes by clicking Fetch origin\n\nEnter a summary and description of the changes\n\nClick Commit to main\nClick Push origin to push committed changes to the GitHub repo\nChanges are now version controlled on GitHub!\n\nBy working locally and frequently pushing changes, you can collaborate with others through the central GitHub repository.\n\n\n\n\n16.1.2 Local Development Workflow\nNic shares the typical workflow for version control - working locally on your computer, committing changes, and pushing them to Github frequently. He adds some dummy data files and a script to his local copy, then demonstrates committing and pushing changes after each work session. Git tracks differences, not complete files, saving space.\nHe recommends writing the readme early on as an exercise to clearly state project goals upfront. The template readme provides a guide for documenting folder structure and contents.\n\n\n16.1.3 Best Practices in R for Reproducibility\nNic emphasizes using R Projects and the here() package to make analysis reproducible across computers. R Projects root file paths to their folder location. here() constructs paths from that root folder in a platform-agnostic way. Together they avoid manual path setting that breaks on other devices.\nHe demonstrates explicitly calling functions from packages with package::function(). This points to the right package if multiple ones have identically named functions.\nWe learn you can call a package function without loading the library, saving memory and environment clutter. The function will automatically install its package if missing."
  },
  {
    "objectID": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#handling-large-files",
    "href": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#handling-large-files",
    "title": "16  Part 2: Using GitHub Desktop, Local Workflow, R Reproducibility, Zotero",
    "section": "16.2 Handling Large Files",
    "text": "16.2 Handling Large Files\nGithub has a 100MB file size limit. For public data, use R to directly download files from online repositories into a script instead of bundling them. For private data, upload to Box and download from there. In both cases the analysis stays reproducible."
  },
  {
    "objectID": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#zotero-for-reference-management",
    "href": "workshop-18DEC2023-part2-githubdesktop-reproducibility-zotero.html#zotero-for-reference-management",
    "title": "16  Part 2: Using GitHub Desktop, Local Workflow, R Reproducibility, Zotero",
    "section": "16.3 Zotero for Reference Management",
    "text": "16.3 Zotero for Reference Management\nNic pays for unlimited Zotero storage and sets up group libraries for shared reference databases linked to downloaded PDFs. Even after students leave Macalester, he can maintain the Zotero storage so past students retain access to the literature they collected."
  },
  {
    "objectID": "workshop-18DEC2023-part3-ebooks-quarto.html#creating-ebooks-with-quarto",
    "href": "workshop-18DEC2023-part3-ebooks-quarto.html#creating-ebooks-with-quarto",
    "title": "17  Part 3: Creating E-Books with Quarto and GitHub Pages",
    "section": "17.1 Creating Ebooks with Quarto",
    "text": "17.1 Creating Ebooks with Quarto\nNic has been locally generating HTMLs from his Quarto ebook for several weeks. When he makes his product repository public on Github, others will be able to access the ebook.\nHe suggests making scientific work more “human” by having a radically open workflow that reveals frustrations and mistakes.\nJen suggests rendering files locally and pushing them to Github. Then the index will pull them up in the browser locally, allowing for easier sharing via screensharing without publishing them publicly."
  },
  {
    "objectID": "workshop-18DEC2023-part3-ebooks-quarto.html#coding-is-writing",
    "href": "workshop-18DEC2023-part3-ebooks-quarto.html#coding-is-writing",
    "title": "17  Part 3: Creating E-Books with Quarto and GitHub Pages",
    "section": "17.2 Coding is Writing!",
    "text": "17.2 Coding is Writing!\nCoding is writing. Nic now has a Blair ebook on the open science workflow and has put up style guides and standards like an R code style guide to make collective code more readable. The style guide covers naming functions, code length, commenting, etc. He condensed style guidelines from Google, the tidyverse, and the book “Clean Code.”\nHe emphasizes logging progress and writing daily, even if just a few bullets and sentences. Hava’s daily logs are a good example.\n\n17.2.1 Setting Up Quarto\n\nTo use Quarto on your computer, you need to install a LaTeX distribution called MacTeX. This allows Quarto to run LaTeX in the background to handle typesetting.\nA Quarto book consists of multiple Markdown files that contain the content, plus a _quarto.yml file that specifies metadata and how to structure the Markdown files.\nThe _quarto.yml file uses YAML syntax to define configuration like output format.\n\n\n\n17.2.2 Writing Content\n\nContent for a Quarto book goes in Markdown files with the .qmd extension. These are just plain text files that support Markdown formatting.\nThe first file must be named index.qmd to serve as the landing page.\nAdditional content files can be named anything, like chapter1.qmd or methods.qmd. The _quarto.yml file controls ordering and structure.\n\n\n\n17.2.3 Configuring Output\n\nTo output a standard ebook, list the Markdown files in _quarto.yml in the desired order. Quarto will number chapters automatically.\nCan also define more complex structures with parts, sub-chapters etc.\nSet output: html and output-dir: docs to generate HTML files in a docs/ folder.\n\n\n\n17.2.4 Rendering the Book\n\nOpen one of the Markdown files in RStudio and click “Render”.\nQuarto will process the files and output HTML.\nOn GitHub, the HTML files are combined into an online ebook.\n\n\n\n17.2.5 Example Structure\n\nShowed example of notebook-style dissertation chapter documenting analysis work with logs and code.\nUseful way to capture full record even with dead ends instead of just the final polished manuscript.\nMarkdown/Quarto structure is flexible for different content types."
  },
  {
    "objectID": "workshop-18DEC2023-part4-future-plans-wrapup.html#potential-future-codingwriting-sessions",
    "href": "workshop-18DEC2023-part4-future-plans-wrapup.html#potential-future-codingwriting-sessions",
    "title": "18  Part 4: Future Plans, Suggestions and Wrap-up",
    "section": "18.1 Potential Future Coding/Writing Sessions",
    "text": "18.1 Potential Future Coding/Writing Sessions\nNic asks if anyone is interested in having coding or writing sessions, either online or in-person. He personally doesn’t find them very useful but thinks others might.\nJen describes her experience with Zoom writing sessions - having check-in times, timed writing blocks, sharing writing for feedback, setting goals for next session. She finds them useful for accountability and community. Sessions can be tailored to the group’s needs.\nAdrian describes his experience in Jonathan’s group which had weekly check-ins, quarterly offsite work retreats, and good collaboration. Adrian sees value in getting peer feedback and help. Jane’s group does not have any regular sessions currently.\nNic suggests trying a small weekly Zoom session for the spring with a Pomodoro timer. It could help collectively solve problems. He asks for input, particularly since he likely won’t be as available to help with coding issues."
  },
  {
    "objectID": "workshop-18DEC2023-part4-future-plans-wrapup.html#collaborative-platforms",
    "href": "workshop-18DEC2023-part4-future-plans-wrapup.html#collaborative-platforms",
    "title": "18  Part 4: Future Plans, Suggestions and Wrap-up",
    "section": "18.2 Collaborative Platforms",
    "text": "18.2 Collaborative Platforms\nNic and Nora have been testing Posit Cloud, a collaborative scripting platform. It allows simultaneous editing of the same R script. Changes sync through Github. This avoids issues with one person screensharing and coding while the other watches. However, some kinks still need to be worked out.\nAdrian suggests Google Colab for Python, which also enables live collaboration through Jupiter Notebook. Nora indicates it can sync with Git. Adrian says Colab has version history like Google Docs."
  },
  {
    "objectID": "workshop-18DEC2023-part4-future-plans-wrapup.html#open-science-ebook-updates",
    "href": "workshop-18DEC2023-part4-future-plans-wrapup.html#open-science-ebook-updates",
    "title": "18  Part 4: Future Plans, Suggestions and Wrap-up",
    "section": "18.3 Open Science Ebook Updates",
    "text": "18.3 Open Science Ebook Updates\nNic plans to continue updating his open science ebook over break. He reflects on “pre-data” collection structures to facilitate projects before data is collected. He realizes most students are too far along to reconstruct things but aims to help future students.\nHe plans to add content on publishing reproducible science, archiving/storing data, licenses, formats, etc. He’ll include more style guidelines like Markdown and tabular data standards.\nHe put personal reflections on what he’s learned from struggles with writing practice and productivity the last two years. He dumped out everything he could to perhaps help others."
  },
  {
    "objectID": "workshop-18DEC2023-part4-future-plans-wrapup.html#using-obsidian",
    "href": "workshop-18DEC2023-part4-future-plans-wrapup.html#using-obsidian",
    "title": "18  Part 4: Future Plans, Suggestions and Wrap-up",
    "section": "18.4 Using Obsidian",
    "text": "18.4 Using Obsidian\nObsidian is an interconnected notes system that’s like a personal Wikipedia. It uses markdown text documents that link to each other. It helps cite sources while writing. The graph view can show linked note clusters. Mentions of a term across notes can be pulled into a single note.\nObsidian is free, open source, platform agnostic, and future proof - the underlying text files can outlast the app. Proprietary tools like Evernote risk losing data if they cease support.\nThe vault contains all notes in one place instead of a hierarchical folder structure. Nicholas uses iCloud to sync his vault across devices."
  },
  {
    "objectID": "workshop-18DEC2023-part4-future-plans-wrapup.html#wrap-up",
    "href": "workshop-18DEC2023-part4-future-plans-wrapup.html#wrap-up",
    "title": "18  Part 4: Future Plans, Suggestions and Wrap-up",
    "section": "18.5 Wrap Up",
    "text": "18.5 Wrap Up\nNic will send a summary with links. He requests adopting the Github project structure moving forward. Regularly pushing changes helps make version control habitual.\nHe thanks Adrian for joining remotely and ends the recording."
  }
]